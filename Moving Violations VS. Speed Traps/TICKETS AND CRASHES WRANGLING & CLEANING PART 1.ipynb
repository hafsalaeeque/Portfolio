{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from datetime import date, timedelta\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data taken from Opendata.dc.gov\n",
    "crash = pd.read_csv('Data/Crashes_in_the_district_of_Columbia.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>﻿X</th>\n",
       "      <th>Y</th>\n",
       "      <th>CRASHID</th>\n",
       "      <th>CRIMEID</th>\n",
       "      <th>ISREPORTONSCENE</th>\n",
       "      <th>WASMAJORCRASHNOTIFIED</th>\n",
       "      <th>MAJORCRASHNOTIFIEDDATE</th>\n",
       "      <th>MAJORCRASHNOTIFIEDPERSONID</th>\n",
       "      <th>SCHOOLBUSRELATED</th>\n",
       "      <th>ISJUNCTIONINTERCHANGEAREA</th>\n",
       "      <th>...</th>\n",
       "      <th>PEDESTRIANSINVOLVED</th>\n",
       "      <th>MINORINJURIES</th>\n",
       "      <th>MAJORINJURIES</th>\n",
       "      <th>FATALITIES</th>\n",
       "      <th>TRAFFICCONTROLDEVICES</th>\n",
       "      <th>ADDRESS_ID</th>\n",
       "      <th>STREETSEGID</th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>XCOORD</th>\n",
       "      <th>YCOORD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-76.931799</td>\n",
       "      <td>38.888110</td>\n",
       "      <td>284561699.0</td>\n",
       "      <td>284564121.0</td>\n",
       "      <td>1</td>\n",
       "      <td>null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16410.0</td>\n",
       "      <td>9948.0</td>\n",
       "      <td>1</td>\n",
       "      <td>405917.240000</td>\n",
       "      <td>135586.72000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>284558848.0</td>\n",
       "      <td>284558858.0</td>\n",
       "      <td>0</td>\n",
       "      <td>null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-76.973458</td>\n",
       "      <td>38.933115</td>\n",
       "      <td>284558851.0</td>\n",
       "      <td>284558872.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>904915.0</td>\n",
       "      <td>2228.0</td>\n",
       "      <td>3</td>\n",
       "      <td>402301.519392</td>\n",
       "      <td>140580.85136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-76.989447</td>\n",
       "      <td>38.927668</td>\n",
       "      <td>284558855.0</td>\n",
       "      <td>177355172.0</td>\n",
       "      <td>1</td>\n",
       "      <td>null</td>\n",
       "      <td>NaN</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>803716.0</td>\n",
       "      <td>3325.0</td>\n",
       "      <td>4</td>\n",
       "      <td>400915.220000</td>\n",
       "      <td>139975.86000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-77.044894</td>\n",
       "      <td>38.906458</td>\n",
       "      <td>284558864.0</td>\n",
       "      <td>284558871.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>null</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>812026.0</td>\n",
       "      <td>10320.0</td>\n",
       "      <td>5</td>\n",
       "      <td>396106.240000</td>\n",
       "      <td>137622.30000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          ﻿X          Y      CRASHID      CRIMEID ISREPORTONSCENE  \\\n",
       "0 -76.931799  38.888110  284561699.0  284564121.0               1   \n",
       "1        NaN        NaN  284558848.0  284558858.0               0   \n",
       "2 -76.973458  38.933115  284558851.0  284558872.0               1   \n",
       "3 -76.989447  38.927668  284558855.0  177355172.0               1   \n",
       "4 -77.044894  38.906458  284558864.0  284558871.0               1   \n",
       "\n",
       "  WASMAJORCRASHNOTIFIED MAJORCRASHNOTIFIEDDATE MAJORCRASHNOTIFIEDPERSONID  \\\n",
       "0                  null                    NaN                       null   \n",
       "1                  null                    NaN                       null   \n",
       "2                     0                    NaN                       null   \n",
       "3                  null                    NaN                       null   \n",
       "4                     0                    NaN                       null   \n",
       "\n",
       "  SCHOOLBUSRELATED ISJUNCTIONINTERCHANGEAREA      ...       \\\n",
       "0             null                      null      ...        \n",
       "1             null                      null      ...        \n",
       "2             null                      null      ...        \n",
       "3             null                      null      ...        \n",
       "4             null                      null      ...        \n",
       "\n",
       "  PEDESTRIANSINVOLVED MINORINJURIES MAJORINJURIES FATALITIES  \\\n",
       "0                 NaN           NaN           NaN        NaN   \n",
       "1                 NaN           NaN           NaN        NaN   \n",
       "2                 NaN           NaN           1.0        NaN   \n",
       "3                 NaN           NaN           NaN        NaN   \n",
       "4                 NaN           NaN           NaN        NaN   \n",
       "\n",
       "   TRAFFICCONTROLDEVICES ADDRESS_ID STREETSEGID OBJECTID         XCOORD  \\\n",
       "0                    NaN    16410.0      9948.0        1  405917.240000   \n",
       "1                    NaN        NaN         NaN        2            NaN   \n",
       "2                    NaN   904915.0      2228.0        3  402301.519392   \n",
       "3                    NaN   803716.0      3325.0        4  400915.220000   \n",
       "4                    NaN   812026.0     10320.0        5  396106.240000   \n",
       "\n",
       "         YCOORD  \n",
       "0  135586.72000  \n",
       "1           NaN  \n",
       "2  140580.85136  \n",
       "3  139975.86000  \n",
       "4  137622.30000  \n",
       "\n",
       "[5 rows x 64 columns]"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#there were a ton of columns that were completely null.\n",
    "crash.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Columns I am actually interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crashs= crash[['\\xef\\xbb\\xbfX', 'Y', 'CRASHID', 'STREETLIGHTING', 'LIGHTCONDITION', 'WEATHER', 'REPORTDATE', \n",
    "               'CYCLISTSINVOLVED', 'PEDESTRIANSINVOLVED', 'MINORINJURIES', 'MAJORINJURIES', 'FATALITIES' ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Cleansing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Filling nan values \n",
    "crashs['FATALITIES'].fillna(value = 0, inplace = True)\n",
    "crashs['MAJORINJURIES'].fillna(value = 0, inplace = True)\n",
    "crashs['MINORINJURIES'].fillna(value = 0, inplace = True)\n",
    "crashs['PEDESTRIANSINVOLVED'].fillna(value = 0, inplace = True)\n",
    "crashs['CYCLISTSINVOLVED'].fillna(value = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Renaming X and Y which are actually Longitude and Latitude\n",
    "crashs.rename(columns = {'\\xef\\xbb\\xbfX': 'LONGITUDE', 'Y':'LATITUDE'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelstack/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# dropping observations where the report data was not recorded\n",
    "crashs.dropna(subset=['REPORTDATE'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# saving resulted dataframe\n",
    "crashs.to_csv('Crashes_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing some scraggly data from the date object\n",
    "date_list = crashs['REPORTDATE'].tolist()\n",
    "\n",
    "new_date = []\n",
    "for date in date_list:\n",
    "    new = date.replace('T', ' ' )\n",
    "    new = new.replace('Z', '')\n",
    "    new_date.append(new)\n",
    "crashs['DATE']=new_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crashs.drop('REPORTDATE', inplace = True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelstack/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "crashs['EVENT'] = 'CRASH'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Crash dataframe for a Granger Causaility Test.\n",
    "GCT takes two timeseries variables or features and compares them with lag to see if one has an affect on the other.\n",
    "I will run a GCT on every unique Locations so I will need both Crashes and Tickets as my features to compare, Data to represent my timeseries and location information so i can create unique locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gra_crash = crashs[['LONGITUDE','LATITUDE','DATE','EVENT']]\n",
    "#gra_crash[['LONGITUDE','LATITUDE']]=gra_crash[['LONGITUDE','LATITUDE']].round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelstack/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "gra_crash.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>DATE</th>\n",
       "      <th>EVENT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-76.9318</td>\n",
       "      <td>38.8881</td>\n",
       "      <td>2009-01-11 00:00:00.000</td>\n",
       "      <td>CRASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-76.9735</td>\n",
       "      <td>38.9331</td>\n",
       "      <td>2009-01-05 00:00:00.000</td>\n",
       "      <td>CRASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-77.0449</td>\n",
       "      <td>38.9065</td>\n",
       "      <td>2008-08-03 00:00:00.000</td>\n",
       "      <td>CRASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-76.9366</td>\n",
       "      <td>38.8869</td>\n",
       "      <td>2008-08-15 00:00:00.000</td>\n",
       "      <td>CRASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-77.0341</td>\n",
       "      <td>38.9274</td>\n",
       "      <td>2009-06-26 00:00:00.000</td>\n",
       "      <td>CRASH</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LONGITUDE  LATITUDE                     DATE  EVENT\n",
       "0   -76.9318   38.8881  2009-01-11 00:00:00.000  CRASH\n",
       "2   -76.9735   38.9331  2009-01-05 00:00:00.000  CRASH\n",
       "4   -77.0449   38.9065  2008-08-03 00:00:00.000  CRASH\n",
       "5   -76.9366   38.8869  2008-08-15 00:00:00.000  CRASH\n",
       "6   -77.0341   38.9274  2009-06-26 00:00:00.000  CRASH"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gra_crash.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A formal data dictionary describing what each of the values means was not provided.  \n",
    "These are my best guesses.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Insights:\n",
    "- Of the 150,000 crashes, only 220 of them confirm that alcohol was a factor. \n",
    "- DC has its own coordinate system because Longitude and latitude were not good enough.\n",
    "- This set of information contains an exceptionally high amount columns that are completely null and columns that are mostly null.  Its like they are trying to make their data look like more than it actually is.\n",
    "- All my Moving violations data is somewhere between 1100mb and 1200mb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelstack/anaconda/lib/python2.7/site-packages/IPython/core/interactiveshell.py:2723: DtypeWarning: Columns (18) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# Initially I planned as possibly encorporating this in as a factor to predict areas with crashs.\n",
    "em_work = pd.read_csv('Data/Emergency_Work_Requests_via_DDOT_TOPs.csv')\n",
    "# I have not found a practical way to encorporate this yet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This was suppose to be reports by residents of DC of safety concerns/issues that could also have some end analysis implications.\n",
    "vzs = pd.read_csv('Data/Vision_Zero_Safety_Transportation.csv')\n",
    "# I probably wont we able to incorporate this for time reasons.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the ticket data came in monthly Increments and had to be merged by hand (aka Python)\n",
    "Fair Warning it is a long list of the same exact thing for 7 years."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2009"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import\n",
    "jan2009 = pd.read_csv('Data/Moving Violation data/01 January/Moving_Violations_in_January_2009.csv')\n",
    "feb2009 = pd.read_csv('Data/Moving Violation data/02 February/Moving_Violations_in_February_2009.csv')\n",
    "mar2009 = pd.read_csv('Data/Moving Violation data/03 March/Moving_Violations_in_March_2009.csv')\n",
    "apr2009 = pd.read_csv('Data/Moving Violation data/04 April/Moving_Violations_in_April_2009.csv')\n",
    "may2009 = pd.read_csv('Data/Moving Violation data/05 May/Moving_Violations_in_May_2009.csv')\n",
    "jun2009 = pd.read_csv('Data/Moving Violation data/06 June/Moving_Violations_in_June_2009.csv')\n",
    "jul2009 = pd.read_csv('Data/Moving Violation data/07 July/Moving_Violations_in_July_2009.csv')\n",
    "aug2009 = pd.read_csv('Data/Moving Violation data/08 August/Moving_Violations_in_August_2009.csv')\n",
    "sep2009 = pd.read_csv('Data/Moving Violation data/09 September/Moving_Violations_in_September_2009.csv')\n",
    "oct2009 = pd.read_csv('Data/Moving Violation data/10 October/Moving_Violations_in_October_2009.csv')\n",
    "nov2009 = pd.read_csv('Data/Moving Violation data/11 November/Moving_Violations_in_November_2009.csv')\n",
    "dec2009 = pd.read_csv('Data/Moving Violation data/12 December/Moving_Violations_in_December_2009.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['\\xef\\xbb\\xbfX', 'Y', 'OBJECTID', 'ROW_', 'LOCATION', 'ADDRESS_ID',\n",
       "       'STREETSEGID', 'XCOORD', 'YCOORD', 'TICKETTYPE', 'FINEAMT',\n",
       "       'TOTALPAID', 'PENALTY1', 'PENALTY2', 'ACCIDENTINDICATOR',\n",
       "       'AGENCYID', 'TICKETISSUEDATE', 'VIOLATIONCODE', 'VIOLATIONDESC',\n",
       "       'ROW_ID'], dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jan2009.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframes2009 = [JAN2009, FEB2009, MAR2009, APR2009, MAY2009, JUN2009, JUL2009, AUG2009, SEP2009, OCT2009, NOV2009, DEC2009]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Desired Fields (Granger Causaility Test)\n",
    "JAN2009 = jan2009[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "FEB2009 = feb2009[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "MAR2009 = mar2009[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "APR2009 = apr2009[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "MAY2009 = may2009[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "JUN2009 = jun2009[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "JUL2009 = jul2009[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "AUG2009 = aug2009[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "SEP2009 = sep2009[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "OCT2009 = oct2009[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "NOV2009 = nov2009[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "DEC2009 = dec2009[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2009_1 = JAN2009.append([FEB2009,MAR2009,APR2009])\n",
    "df2009_2 = MAY2009.append([JUN2009,JUL2009,AUG2009])\n",
    "df2009_3 = SEP2009.append([OCT2009,NOV2009,DEC2009])\n",
    "df2009 = df2009_1.append([df2009_2, df2009_3])\n",
    "df2009.to_csv('moving_violations2009.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doing the same cleaning for every frame and feature engineering would be a drag.  Created a sweet function to do things for me."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def is_trap_f(df):\n",
    "    is_trap =[]\n",
    "    ticket_type = df['TICKETTYPE'].tolist()\n",
    "    for ticket in ticket_type:\n",
    "        if ticket == 'Photo':\n",
    "            is_trap.append(1)\n",
    "        else:\n",
    "            is_trap.append(0)\n",
    "    df['TRAP'] =is_trap\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelstack/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "for year in dataframes2009:\n",
    "    is_trap_f(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for year in dataframes2009:\n",
    "    year.rename(columns ={'\\xef\\xbb\\xbfX': 'LONGITUDE', 'Y': 'LATITUDE'}, inplace = True)\n",
    "    year[['LONGITUDE',\"LATITUDE\"]] = year[['LONGITUDE',\"LATITUDE\"]].round(3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2010\n",
    "The 2010 December csv on opendata.dc.gov had some issues when i was concatting by year and I could not download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jan2010 = pd.read_csv('Data/Moving Violation data/01 January/Moving_Violations_in_January_2010.csv')\n",
    "feb2010 = pd.read_csv('Data/Moving Violation data/02 February/Moving_Violations_in_February_2010.csv')\n",
    "mar2010 = pd.read_csv('Data/Moving Violation data/03 March/Moving_Violations_in_March_2010.csv')\n",
    "apr2010 = pd.read_csv('Data/Moving Violation data/04 April/Moving_Violations_in_April_2010.csv')\n",
    "may2010 = pd.read_csv('Data/Moving Violation data/05 May/Moving_Violations_in_May_2010.csv')\n",
    "jun2010 = pd.read_csv('Data/Moving Violation data/06 June/Moving_Violations_in_June_2010.csv')\n",
    "jul2010 = pd.read_csv('Data/Moving Violation data/07 July/Moving_Violations_in_July_2010.csv')\n",
    "aug2010 = pd.read_csv('Data/Moving Violation data/08 August/Moving_Violations_in_August_2010.csv')\n",
    "sep2010 = pd.read_csv('Data/Moving Violation data/09 September/Moving_Violations_in_September_2010.csv')\n",
    "oct2010 = pd.read_csv('Data/Moving Violation data/10 October/Moving_Violations_in_October_2010.csv')\n",
    "nov2010 = pd.read_csv('Data/Moving Violation data/11 November/Moving_Violations_in_November_2010.csv')\n",
    "#dec2010 = pd.read_csv('Data/Moving Violation data/12 December/Moving_Violations_in_December_2010.csv') This dataset is not downloadable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Desired Fields (Granger Causaility Test)\n",
    "JAN2010 = jan2010[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "FEB2010 = feb2010[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "MAR2010 = mar2010[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "APR2010 = apr2010[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "MAY2010 = may2010[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "JUN2010 = jun2010[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "JUL2010 = jul2010[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "AUG2010 = aug2010[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "SEP2010 = sep2010[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "OCT2010 = oct2010[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "NOV2010 = nov2010[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "#DEC2010 = dec2010[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframes2010 = [JAN2010, FEB2010, MAR2010, APR2010, MAY2010, JUN2010, JUL2010, AUG2010, SEP2010, OCT2010, NOV2010]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelstack/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "for year in dataframes2010:\n",
    "    is_trap_f(year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for year in dataframes2010:\n",
    "    year.rename(columns ={'\\xef\\xbb\\xbfX': 'LONGITUDE', 'Y': 'LATITUDE'}, inplace = True)\n",
    "    year[['LONGITUDE',\"LATITUDE\"]] = year[['LONGITUDE',\"LATITUDE\"]].round(3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df2010_1 = JAN2010.append([FEB2010,MAR2010,APR2010])\n",
    "df2010_2 = MAY2010.append([JUN2010,JUL2010,AUG2010])\n",
    "df2010_3 = SEP2010.append([OCT2010,NOV2010])\n",
    "df2010 = df2010_1.append([df2010_2, df2010_3])\n",
    "df2010.to_csv('moving_violations2010.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jan2011 = pd.read_csv('Data/Moving Violation data/01 January/Moving_Violations_in_January_2011.csv')\n",
    "feb2011 = pd.read_csv('Data/Moving Violation data/02 February/Moving_Violations_in_February_2011.csv')\n",
    "mar2011 = pd.read_csv('Data/Moving Violation data/03 March/Moving_Violations_in_March_2011.csv')\n",
    "apr2011 = pd.read_csv('Data/Moving Violation data/04 April/Moving_Violations_in_April_2011.csv')\n",
    "may2011 = pd.read_csv('Data/Moving Violation data/05 May/Moving_Violations_in_May_2011.csv')\n",
    "jun2011 = pd.read_csv('Data/Moving Violation data/06 June/Moving_Violations_in_June_2011.csv')\n",
    "jul2011 = pd.read_csv('Data/Moving Violation data/07 July/Moving_Violations_in_July_2011.csv')\n",
    "aug2011 = pd.read_csv('Data/Moving Violation data/08 August/Moving_Violations_in_August_2011.csv')\n",
    "sep2011 = pd.read_csv('Data/Moving Violation data/09 September/Moving_Violations_in_September_2011.csv')\n",
    "oct2011 = pd.read_csv('Data/Moving Violation data/10 October/Moving_Violations_in_October_2011.csv')\n",
    "nov2011 = pd.read_csv('Data/Moving Violation data/11 November/Moving_Violations_in_November_2011.csv')\n",
    "dec2011 = pd.read_csv('Data/Moving Violation data/12 December/Moving_Violations_in_December_2011.csv')\n",
    "\n",
    "# Desired Fields (Granger Causaility Test)\n",
    "JAN2011 = jan2011[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "FEB2011 = feb2011[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "MAR2011 = mar2011[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "APR2011 = apr2011[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "MAY2011 = may2011[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "JUN2011 = jun2011[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "JUL2011 = jul2011[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "AUG2011 = aug2011[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "SEP2011 = sep2011[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "OCT2011 = oct2011[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "NOV2011 = nov2011[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "DEC2011 = dec2011[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "\n",
    "dataframes2011 = [JAN2011, FEB2011, MAR2011, APR2011, MAY2011, JUN2011, JUL2011, AUG2011, SEP2011, OCT2011, NOV2011, DEC2011]\n",
    "\n",
    "# for year in dataframes2011:\n",
    "#     is_trap_f(year)\n",
    "    \n",
    "for year in dataframes2011:\n",
    "    year.rename(columns ={'\\xef\\xbb\\xbfX': 'LONGITUDE', 'Y': 'LATITUDE'}, inplace = True)\n",
    "    year[['LONGITUDE',\"LATITUDE\"]] = year[['LONGITUDE',\"LATITUDE\"]].round(3) \n",
    "    \n",
    "df2011_1 = JAN2011.append([FEB2011,MAR2011,APR2011])\n",
    "df2011_2 = MAY2011.append([JUN2011,JUL2011,AUG2011])\n",
    "df2011_3 = SEP2011.append([OCT2011,NOV2011,DEC2011])\n",
    "df2011 = df2011_1.append([df2011_2, df2011_3])\n",
    "df2011.to_csv('moving_violations2011.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>TICKETTYPE</th>\n",
       "      <th>TICKETISSUEDATE</th>\n",
       "      <th>FINEAMT</th>\n",
       "      <th>ACCIDENTINDICATOR</th>\n",
       "      <th>VIOLATIONDESC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-77.006</td>\n",
       "      <td>38.927</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2011-01-28T00:00:00.000Z</td>\n",
       "      <td>125.0</td>\n",
       "      <td>No</td>\n",
       "      <td>SPEED 11-15 MPH OVER THE SPEED LIMIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-76.995</td>\n",
       "      <td>38.879</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2011-01-28T00:00:00.000Z</td>\n",
       "      <td>125.0</td>\n",
       "      <td>No</td>\n",
       "      <td>SPEED 11-15 MPH OVER THE SPEED LIMIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-76.996</td>\n",
       "      <td>38.912</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2011-01-28T00:00:00.000Z</td>\n",
       "      <td>125.0</td>\n",
       "      <td>No</td>\n",
       "      <td>SPEED 11-15 MPH OVER THE SPEED LIMIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-76.965</td>\n",
       "      <td>38.897</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2011-01-30T00:00:00.000Z</td>\n",
       "      <td>200.0</td>\n",
       "      <td>No</td>\n",
       "      <td>SPEED 21-25 MPH OVER THE SPEED LIMIT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-76.996</td>\n",
       "      <td>38.912</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2011-01-30T00:00:00.000Z</td>\n",
       "      <td>125.0</td>\n",
       "      <td>No</td>\n",
       "      <td>SPEED 11-15 MPH OVER THE SPEED LIMIT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   LONGITUDE  LATITUDE TICKETTYPE           TICKETISSUEDATE  FINEAMT  \\\n",
       "0    -77.006    38.927      Photo  2011-01-28T00:00:00.000Z    125.0   \n",
       "1    -76.995    38.879      Photo  2011-01-28T00:00:00.000Z    125.0   \n",
       "2    -76.996    38.912      Photo  2011-01-28T00:00:00.000Z    125.0   \n",
       "3    -76.965    38.897      Photo  2011-01-30T00:00:00.000Z    200.0   \n",
       "4    -76.996    38.912      Photo  2011-01-30T00:00:00.000Z    125.0   \n",
       "\n",
       "  ACCIDENTINDICATOR                         VIOLATIONDESC  \n",
       "0                No  SPEED 11-15 MPH OVER THE SPEED LIMIT  \n",
       "1                No  SPEED 11-15 MPH OVER THE SPEED LIMIT  \n",
       "2                No  SPEED 11-15 MPH OVER THE SPEED LIMIT  \n",
       "3                No  SPEED 21-25 MPH OVER THE SPEED LIMIT  \n",
       "4                No  SPEED 11-15 MPH OVER THE SPEED LIMIT  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2011.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jan2012 = pd.read_csv('Data/Moving Violation data/01 January/Moving_Violations_in_January_2012.csv')\n",
    "feb2012 = pd.read_csv('Data/Moving Violation data/02 February/Moving_Violations_in_February_2012.csv')\n",
    "mar2012 = pd.read_csv('Data/Moving Violation data/03 March/Moving_Violations_in_March_2012.csv')\n",
    "apr2012 = pd.read_csv('Data/Moving Violation data/04 April/Moving_Violations_in_April_2012.csv')\n",
    "may2012 = pd.read_csv('Data/Moving Violation data/05 May/Moving_Violations_in_May_2012.csv')\n",
    "jun2012 = pd.read_csv('Data/Moving Violation data/06 June/Moving_Violations_in_June_2012.csv')\n",
    "jul2012 = pd.read_csv('Data/Moving Violation data/07 July/Moving_Violations_in_July_2012.csv')\n",
    "aug2012 = pd.read_csv('Data/Moving Violation data/08 August/Moving_Violations_in_August_2012.csv')\n",
    "sep2012 = pd.read_csv('Data/Moving Violation data/09 September/Moving_Violations_in_September_2012.csv')\n",
    "oct2012 = pd.read_csv('Data/Moving Violation data/10 October/Moving_Violations_in_October_2012.csv')\n",
    "nov2012 = pd.read_csv('Data/Moving Violation data/11 November/Moving_Violations_in_November_2012.csv')\n",
    "dec2012 = pd.read_csv('Data/Moving Violation data/12 December/Moving_Violations_in_December_2012.csv')\n",
    "\n",
    "# Desired Fields (Granger Causaility Test)\n",
    "JAN2012 = jan2012[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "FEB2012 = feb2012[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "MAR2012 = mar2012[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "APR2012 = apr2012[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "MAY2012 = may2012[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "JUN2012 = jun2012[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "JUL2012 = jul2012[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "AUG2012 = aug2012[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "SEP2012 = sep2012[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "OCT2012 = oct2012[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "NOV2012 = nov2012[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "DEC2012 = dec2012[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "\n",
    "dataframes2012 = [JAN2012, FEB2012, MAR2012, APR2012, MAY2012, JUN2012, JUL2012, AUG2012, SEP2012, OCT2012, NOV2012, DEC2012]\n",
    "\n",
    "# for year in dataframes2012:\n",
    "#     is_trap_f(year)\n",
    "    \n",
    "for year in dataframes2012:\n",
    "    year.rename(columns ={'\\xef\\xbb\\xbfX': 'LONGITUDE', 'Y': 'LATITUDE'}, inplace = True)\n",
    "    year[['LONGITUDE',\"LATITUDE\"]] = year[['LONGITUDE',\"LATITUDE\"]].round(3) \n",
    "    \n",
    "df2012_1 = JAN2012.append([FEB2012,MAR2012,APR2012])\n",
    "df2012_2 = MAY2012.append([JUN2012,JUL2012,AUG2012])\n",
    "df2012_3 = SEP2012.append([OCT2012,NOV2012,DEC2012])\n",
    "df2012 = df2012_1.append([df2012_2, df2012_3])\n",
    "df2012.to_csv('moving_violations2012.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jan2013 = pd.read_csv('Data/Moving Violation data/01 January/Moving_Violations_in_January_2013.csv')\n",
    "feb2013 = pd.read_csv('Data/Moving Violation data/02 February/Moving_Violations_in_February_2013.csv')\n",
    "mar2013 = pd.read_csv('Data/Moving Violation data/03 March/Moving_Violations_in_March_2013.csv')\n",
    "apr2013 = pd.read_csv('Data/Moving Violation data/04 April/Moving_Violations_in_April_2013.csv')\n",
    "may2013 = pd.read_csv('Data/Moving Violation data/05 May/Moving_Violations_in_May_2013.csv')\n",
    "jun2013 = pd.read_csv('Data/Moving Violation data/06 June/Moving_Violations_in_June_2013.csv')\n",
    "jul2013 = pd.read_csv('Data/Moving Violation data/07 July/Moving_Violations_in_July_2013.csv')\n",
    "aug2013 = pd.read_csv('Data/Moving Violation data/08 August/Moving_Violations_in_August_2013.csv')\n",
    "sep2013 = pd.read_csv('Data/Moving Violation data/09 September/Moving_Violations_in_September_2013.csv')\n",
    "oct2013 = pd.read_csv('Data/Moving Violation data/10 October/Moving_Violations_in_October_2013.csv')\n",
    "nov2013 = pd.read_csv('Data/Moving Violation data/11 November/Moving_Violations_in_November_2013.csv')\n",
    "dec2013 = pd.read_csv('Data/Moving Violation data/12 December/Moving_Violations_in_December_2013.csv')\n",
    "\n",
    "# Desired Fields (Granger Causaility Test)\n",
    "JAN2013 = jan2013[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "FEB2013 = feb2013[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "MAR2013 = mar2013[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "APR2013 = apr2013[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "MAY2013 = may2013[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "JUN2013 = jun2013[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "JUL2013 = jul2013[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "AUG2013 = aug2013[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "SEP2013 = sep2013[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "OCT2013 = oct2013[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "NOV2013 = nov2013[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "DEC2013 = dec2013[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "\n",
    "dataframes2013 = [JAN2013, FEB2013, MAR2013, APR2013, MAY2013, JUN2013, JUL2013, AUG2013, SEP2013, OCT2013, NOV2013, DEC2013]\n",
    "\n",
    "# for year in dataframes2013:\n",
    "#     is_trap_f(year)\n",
    "    \n",
    "for year in dataframes2013:\n",
    "    year.rename(columns ={'\\xef\\xbb\\xbfX': 'LONGITUDE', 'Y': 'LATITUDE'}, inplace = True)\n",
    "    year[['LONGITUDE',\"LATITUDE\"]] = year[['LONGITUDE',\"LATITUDE\"]].round(3) \n",
    "    \n",
    "df2013_1 = JAN2013.append([FEB2013,MAR2013,APR2013])\n",
    "df2013_2 = MAY2013.append([JUN2013,JUL2013,AUG2013])\n",
    "df2013_3 = SEP2013.append([OCT2013,NOV2013,DEC2013])\n",
    "df2013 = df2013_1.append([df2013_2, df2013_3])\n",
    "df2013.to_csv('moving_violations2013.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jan2014 = pd.read_csv('Data/Moving Violation data/01 January/Moving_Violations_in_January_2014.csv')\n",
    "feb2014 = pd.read_csv('Data/Moving Violation data/02 February/Moving_Violations_in_February_2014.csv')\n",
    "mar2014 = pd.read_csv('Data/Moving Violation data/03 March/Moving_Violations_in_March_2014.csv')\n",
    "apr2014 = pd.read_csv('Data/Moving Violation data/04 April/Moving_Violations_in_April_2014.csv')\n",
    "may2014 = pd.read_csv('Data/Moving Violation data/05 May/Moving_Violations_in_May_2014.csv')\n",
    "jun2014 = pd.read_csv('Data/Moving Violation data/06 June/Moving_Violations_in_June_2014.csv')\n",
    "jul2014 = pd.read_csv('Data/Moving Violation data/07 July/Moving_Violations_in_July_2014.csv')\n",
    "aug2014 = pd.read_csv('Data/Moving Violation data/08 August/Moving_Violations_in_August_2014.csv')\n",
    "sep2014 = pd.read_csv('Data/Moving Violation data/09 September/Moving_Violations_in_September_2014.csv')\n",
    "oct2014 = pd.read_csv('Data/Moving Violation data/10 October/Moving_Violations_in_October_2014.csv')\n",
    "nov2014 = pd.read_csv('Data/Moving Violation data/11 November/Moving_Violations_in_November_2014.csv')\n",
    "dec2014 = pd.read_csv('Data/Moving Violation data/12 December/Moving_Violations_in_December_2014.csv')\n",
    "\n",
    "# Desired Fields (Granger Causaility Test)\n",
    "JAN2014 = jan2014[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "FEB2014 = feb2014[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "MAR2014 = mar2014[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "APR2014 = apr2014[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "MAY2014 = may2014[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "JUN2014 = jun2014[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "JUL2014 = jul2014[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "AUG2014 = aug2014[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "SEP2014 = sep2014[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "OCT2014 = oct2014[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "NOV2014 = nov2014[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "DEC2014 = dec2014[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "\n",
    "dataframes2014 = [JAN2014, FEB2014, MAR2014, APR2014, MAY2014, JUN2014, JUL2014, AUG2014, SEP2014, OCT2014, NOV2014, DEC2014]\n",
    "\n",
    "# for year in dataframes2014:\n",
    "#     is_trap_f(year)\n",
    "    \n",
    "for year in dataframes2014:\n",
    "    year.rename(columns ={'\\xef\\xbb\\xbfX': 'LONGITUDE', 'Y': 'LATITUDE'}, inplace = True)\n",
    "    year[['LONGITUDE',\"LATITUDE\"]] = year[['LONGITUDE',\"LATITUDE\"]].round(3) \n",
    "    \n",
    "df2014_1 = JAN2014.append([FEB2014,MAR2014,APR2014])\n",
    "df2014_2 = MAY2014.append([JUN2014,JUL2014,AUG2014])\n",
    "df2014_3 = SEP2014.append([OCT2014,NOV2014,DEC2014])\n",
    "df2014 = df2014_1.append([df2014_2, df2014_3])\n",
    "df2014.to_csv('moving_violations2014.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2015"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jan2015 = pd.read_csv('Data/Moving Violation data/01 January/Moving_Violations_in_January_2015.csv')\n",
    "feb2015 = pd.read_csv('Data/Moving Violation data/02 February/Moving_Violations_in_February_2015.csv')\n",
    "mar2015 = pd.read_csv('Data/Moving Violation data/03 March/Moving_Violations_in_March_2015.csv')\n",
    "apr2015 = pd.read_csv('Data/Moving Violation data/04 April/Moving_Violations_in_April_2015.csv')\n",
    "may2015 = pd.read_csv('Data/Moving Violation data/05 May/Moving_Violations_in_May_2015.csv')\n",
    "jun2015 = pd.read_csv('Data/Moving Violation data/06 June/Moving_Violations_in_June_2015.csv')\n",
    "jul2015 = pd.read_csv('Data/Moving Violation data/07 July/Moving_Violations_in_July_2015.csv')\n",
    "aug2015 = pd.read_csv('Data/Moving Violation data/08 August/Moving_Violations_in_August_2015.csv')\n",
    "sep2015 = pd.read_csv('Data/Moving Violation data/09 September/Moving_Violations_in_September_2015.csv')\n",
    "oct2015 = pd.read_csv('Data/Moving Violation data/10 October/Moving_Violations_in_October_2015.csv')\n",
    "nov2015 = pd.read_csv('Data/Moving Violation data/11 November/Moving_Violations_in_November_2015.csv')\n",
    "dec2015 = pd.read_csv('Data/Moving Violation data/12 December/Moving_Violations_in_December_2015.csv')\n",
    "\n",
    "# Desired Fields (Granger Causaility Test)\n",
    "JAN2015 = jan2015[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "FEB2015 = feb2015[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "MAR2015 = mar2015[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "APR2015 = apr2015[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "MAY2015 = may2015[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "JUN2015 = jun2015[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "JUL2015 = jul2015[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "AUG2015 = aug2015[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "SEP2015 = sep2015[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "OCT2015 = oct2015[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "NOV2015 = nov2015[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "DEC2015 = dec2015[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "\n",
    "dataframes2015 = [JAN2015, FEB2015, MAR2015, APR2015, MAY2015, JUN2015, JUL2015, AUG2015, SEP2015, OCT2015, NOV2015, DEC2015]\n",
    "\n",
    "# for year in dataframes2015:\n",
    "#     is_trap_f(year)\n",
    "    \n",
    "for year in dataframes2015:\n",
    "    year.rename(columns ={'\\xef\\xbb\\xbfX': 'LONGITUDE', 'Y': 'LATITUDE'}, inplace = True)\n",
    "    year[['LONGITUDE',\"LATITUDE\"]] = year[['LONGITUDE',\"LATITUDE\"]].round(3) \n",
    "    \n",
    "df2015_1 = JAN2015.append([FEB2015,MAR2015,APR2015])\n",
    "df2015_2 = MAY2015.append([JUN2015,JUL2015,AUG2015])\n",
    "df2015_3 = SEP2015.append([OCT2015,NOV2015,DEC2015])\n",
    "df2015 = df2015_1.append([df2015_2, df2015_3])\n",
    "df2015.to_csv('moving_violations2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jan2016 = pd.read_csv('Data/Moving Violation data/01 January/Moving_Violations_in_January_2016.csv')\n",
    "feb2016 = pd.read_csv('Data/Moving Violation data/02 February/Moving_Violations_in_February_2016.csv')\n",
    "mar2016 = pd.read_csv('Data/Moving Violation data/03 March/Moving_Violations_in_March_2016.csv')\n",
    "apr2016 = pd.read_csv('Data/Moving Violation data/04 April/Moving_Violations_in_April_2016.csv')\n",
    "may2016 = pd.read_csv('Data/Moving Violation data/05 May/Moving_Violations_in_May_2016.csv')\n",
    "# jun2016 = pd.read_csv('Data/Moving Violation data/06 June/Moving_Violations_in_June_2016.csv')\n",
    "# jul2016 = pd.read_csv('Data/Moving Violation data/07 July/Moving_Violations_in_July_2016.csv')\n",
    "# aug2016 = pd.read_csv('Data/Moving Violation data/08 August/Moving_Violations_in_August_2016.csv')\n",
    "# sep2016 = pd.read_csv('Data/Moving Violation data/09 September/Moving_Violations_in_September_2016.csv')\n",
    "# oct2016 = pd.read_csv('Data/Moving Violation data/10 October/Moving_Violations_in_October_2016.csv')\n",
    "# nov2016 = pd.read_csv('Data/Moving Violation data/11 November/Moving_Violations_in_November_2016.csv')\n",
    "# dec2016 = pd.read_csv('Data/Moving Violation data/12 December/Moving_Violations_in_December_2016.csv')\n",
    "\n",
    "# Desired Fields (Granger Causaility Test)\n",
    "JAN2016 = jan2016[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "FEB2016 = feb2016[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "MAR2016 = mar2016[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "APR2016 = apr2016[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "MAY2016 = may2016[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "# JUN2016 = jun2016[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "# JUL2016 = jul2016[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "# AUG2016 = aug2016[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "# SEP2016 = sep2016[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "# OCT2016 = oct2016[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "# NOV2016 = nov2016[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "# DEC2016 = dec2016[['\\xef\\xbb\\xbfX', 'Y','TICKETTYPE','TICKETISSUEDATE','FINEAMT','ACCIDENTINDICATOR','VIOLATIONDESC']]\n",
    "\n",
    "dataframes2016 = [JAN2016, FEB2016, MAR2016, APR2016, MAY2016]\n",
    "\n",
    "#, JUN2016, JUL2016, AUG2016, SEP2016, OCT2016, NOV2016, DEC2016\n",
    "# for year in dataframes2016:\n",
    "#     is_trap_f(year)\n",
    "    \n",
    "for year in dataframes2016:\n",
    "    year.rename(columns ={'\\xef\\xbb\\xbfX': 'LONGITUDE', 'Y': 'LATITUDE'}, inplace = True)\n",
    "    year[['LONGITUDE',\"LATITUDE\"]] = year[['LONGITUDE',\"LATITUDE\"]].round(3) \n",
    "    \n",
    "df2016_1 = JAN2016.append([FEB2016,MAR2016,APR2016,MAY2016])\n",
    "# df2016_2 = MAY2016.append([JUN2016,JUL2016,AUG2016])\n",
    "# df2016_3 = SEP2016.append([OCT2016,NOV2016,DEC2016])\n",
    "#df2016 = df2016_1.append([df2016_2])\n",
    "df2016_1.to_csv('moving_violations2016.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Because of how longitude and latitude are calculate i decided it be more computationally efficient and accurate to round the Longitude and Latitude coordinates to broader positions.    This measurement is good for identifying intersections or areas on a road.\n",
    "\n",
    "- The sign tells us whether we are north or south, east or west on the globe.\n",
    "- A nonzero hundreds digit tells us we're using longitude, not latitude!\n",
    "- The tens digit gives a position to about 1,000 kilometers. It gives us useful information about what continent or ocean we are on.\n",
    "- The units digit (one decimal degree) gives a position up to 111 kilometers (60 nautical miles, about 69 miles). It can tell us roughly what large state or country we are in.\n",
    "- The first decimal place is worth up to 11.1 km: it can distinguish the position of one large city from a neighboring large city.\n",
    "- The second decimal place is worth up to 1.1 km: it can separate one village from the next.\n",
    "- The third decimal place is worth up to 110 m: it can identify a large agricultural field or institutional campus.\n",
    "##### - The fourth decimal place is worth up to 11 m: it can identify a parcel of land. It is comparable to the typical accuracy of an uncorrected GPS unit with no interference.\n",
    "- The fifth decimal place is worth up to 1.1 m: it distinguish trees from each other. Accuracy to this level with commercial GPS units can only be achieved with differential correction.\n",
    "- The sixth decimal place is worth up to 0.11 m: you can use this for laying out structures in detail, for designing landscapes, building roads. It should be more than good enough for tracking movements of glaciers and rivers. This can be achieved by taking painstaking measures with GPS, such as differentially corrected GPS.\n",
    "- The seventh decimal place is worth up to 11 mm: this is good for much surveying and is near the limit of what GPS-based techniques can achieve.\n",
    "- The eighth decimal place is worth up to 1.1 mm: this is good for charting motions of tectonic plates and movements of volcanoes. Permanent, corrected, constantly-running GPS base stations might be able to achieve this level of accuracy.\n",
    "- The ninth decimal place is worth up to 110 microns: we are getting into the range of microscopy. For almost any conceivable application with earth positions, this is overkill and will be more precise than the accuracy of any surveying device.\n",
    "- Ten or more decimal places indicates a computer or calculator was used and that no attention was paid to the fact that the extra decimals are useless. Be careful, because unless you are the one reading these numbers off the device, this can indicate low quality processing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Insights\n",
    "- T119 --  Speeding 11-15 mph over limit\n",
    "- T120 --  Speeding 16-20 mph over limit\n",
    "- T113 --  Red Light\n",
    "- T121 --  Speeding 21-25 mph over limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#I used this data for a month of Exploratory analysis in Tableau\n",
    "tab_jan = JAN2009[['LONGITUDE','LATITUDE','DATE','EVENT']]\n",
    "tab_crash = crashs[['LONGITUDE','LATITUDE','DATE','EVENT']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tab_data= tab_jan.append(tab_crash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "date_L = tab_data['DATE']\n",
    "dayt = []\n",
    "for time in date_L:\n",
    "    nt = time.replace('.000', '')\n",
    "    dayt.append(nt)\n",
    "    \n",
    "tab_data['DATE1']=dayt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tab_data.drop('DATE', axis = 1, inplace =True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tab_data.drop([6210,114802,7557,63007, 102937, 10669, 32350,76145], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#tab_data.sort(columns = 'DATE')\n",
    "# Issues converting to datatime brought up that there are some invalid dates in these datasets.\n",
    "# 2 crashes from 1908, another from 1975\n",
    "# Also, 5 Crashes that happened in the future, 2017, 2025, 2028, 2320 and 2912\n",
    "# Manually removed these rows, which is the series of index ids in the above cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Converting to datetime \n",
    "t = tab_data['DATE1']\n",
    "tdt = pd.to_datetime(t, format = \"%Y-%m-%d %H:%M:%S\")\n",
    "tab_data['DATE'] = tdt\n",
    "tab_data.drop('DATE1', inplace = True, axis =1)\n",
    "# did this by creating a list, modifying list, appending new list as column and dropping the old one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# As this data is for exploratoy analysis I will drop all null data.\n",
    "tab_data.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Don't have any ticket information prior to 2009 so removing all records that were previous.\n",
    "tab_data = tab_data[(tab_data['DATE'].dt.year >= 2009)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
