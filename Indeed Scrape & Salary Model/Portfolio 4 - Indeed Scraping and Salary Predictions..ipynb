{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective: \n",
    "### Scrape Indeed.com for positions for Data Scientists and try to create a model that can predict whether they will be well paying positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scraping Imports\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indeed Extracting Target Data\n",
    "\n",
    "Company\n",
    "\n",
    "Position Title\n",
    "\n",
    "Location\n",
    "\n",
    "Salary\n",
    "\n",
    "URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Scraper being used \n",
    "def sky_scraper2(url):\n",
    "\n",
    "    city = requests.get(url)\n",
    "    city_data = city.content\n",
    "    soup_city = BeautifulSoup(city_data)\n",
    "    \n",
    "    \n",
    "\n",
    "    for item in soup_city.findAll(\"div\",{\"class\":\" row result\"}):\n",
    "        # Gets the position title\n",
    "        title = item.find(\"a\").get(\"title\")\n",
    "        #Gets the Companies names\n",
    "        try:\n",
    "            company = item.find(name='span', attrs={'itemprop':'name'}).text\n",
    "            company = re.sub('\\s+','',company)\n",
    "            company.strip()\n",
    "        except AttributeError:\n",
    "            company = 'none'\n",
    "        # Gets the positions location and only appends the city name and state abbreviation.     \n",
    "        location = item.find(\"span\", {\"itemprop\" : \"addressLocality\"}).text\n",
    "        location = re.sub(\" \\d+\", \" \", location)\n",
    "        location = re.sub(r'\\([^)]*\\)', '', location)\n",
    "        location = re.sub('\\s+','',location)\n",
    "        \n",
    "        #Gets the positions salary, if the position actually comes with one. \n",
    "        try:\n",
    "            salary = item.find(name =\"nobr\").text\n",
    "        except AttributeError:\n",
    "            salary = 'none'\n",
    "        \n",
    "        # Gets the short description.\n",
    "        description = item.find(\"span\",{\"class\":\"summary\"}).text\n",
    "        \n",
    "        # Gets the url for the link to the actual job posting. \n",
    "        # This is used later to get the positions full description (after cleaning)\n",
    "        direction = item.find(\"a\",{\"rel\":\"nofollow\"}).get(\"href\")\n",
    "        url = 'http://www.indeed.com'+direction\n",
    "        \n",
    "        # appends all the findings to a dataframe.\n",
    "        jobs_df.loc[len(jobs_df)]=[title, company, location, salary, description, url] \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried many things to try to get the full position description using the main scraper, however I encountered many road blocks.\n",
    "\n",
    "1. SSL Errors -  These are thrown when a website believes that whoever is trying to access their site is a bot.  This can be the result of an IP hitting webpages faster than humaly possible or the website detecting that the request is not coming from a browser, in thise case they were coming from python.\n",
    "\n",
    "2.  To avoid the possibility of being denied by the website while using python I set up a Headless Browser using Selenium.  I set Selenium to essentially run through Google Chrome so the requests coming from my function would look like they were coming from Google Chrome rather than Python.\n",
    ">- a.) Selenium actually has to open web pages for every request which made it extreamly slow.\n",
    ">- b.) if a script to close the page once finished was not included, then Selenium could open hundred of pages and                eventually crash your computer.\n",
    ">- c.)  Websites that were not secure often cause the script to freeze without notice or explanation.\n",
    "    (I though i left it running for 2 hours but it froze on a page 5 minutes in)\n",
    "    \n",
    "Eventually, once I dropped all the unusable data a ran another scraper that would just pass up those websites that would not accept requests from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#list of cities used\n",
    "cities = ['New+York', 'Chicago', 'San+Francisco', 'Austin', 'Seattle', \n",
    "   'Los+Angeles', 'Philadelphia', 'Atlanta', 'Dallas', 'Pittsburgh', \n",
    "   'Portland', 'Phoenix', 'Denver', 'Houston', 'Miami', 'Washington']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# defining the dataframe our results will append to.\n",
    "cols = ['title','company','location','salary','description','url']\n",
    "jobs_df = pd.DataFrame(columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Including results for both data scientist and data analyst as broadening my search will help me accumulate more results.\n",
    "scientist_template = \"http://www.indeed.com/jobs?q=data+scientist&l=%s&start=%s&limit=10\"\n",
    "analyst_template = \"http://www.indeed.com/jobs?q=data+analyst&l=%s&start=%s&limit=10\"\n",
    "# how many results you want per each city\n",
    "max_results_per_city = 10\n",
    "\n",
    "\n",
    "# for loop to scrape all cities desired that are Data Scientists or Data Analysts\n",
    "for city in cities:\n",
    "    for start in range(0, max_results_per_city, 10):\n",
    "        for template_url in [scientist_template, analyst_template]:\n",
    "            sky_scraper2(template_url % (city, start))\n",
    "        # Grab the results from the request (as above)\n",
    "        # Append to the full set of results\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As the dataset is rather large and takes a while to aquire it is recommended to save it as a CSV once the scraper finishes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# how to save to csv.\n",
    "jobs_df.to_csv('indeed_jobs.csv',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the save csv, if you need to start over for whatever reason.\n",
    "jsp_df = pd.read_csv('indeed_jobs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Data Cleaning\n",
    "1. Remove Duplicates and null\n",
    "2. Remove all rows where Salary has a 'none' value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Jobs pure has dropped all duplicate values\n",
    "jobs_df.drop_duplicates(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# gets rid of the positions with a salary value of 'none'\n",
    "jobs_df = jobs_df[jobs_df['salary'] != 'none'] \n",
    "# JSP = Jobs Salary Pure dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salary Column Cleaning\n",
    "\n",
    "1. Remove Dollar sign \n",
    "2. Remove everything after dashes.  (I elected to take the lower value in a range as thats what employers are going to try to pay you and try to trick you with a high end of the range.)\n",
    "3. Convert hourly pay to a yearly salary ( 2000 standard work hours in a year),\n",
    "4. Remove all word values ('a month','a year','an hour').\n",
    "5. Remove commas so numbers can be converted to floats.\n",
    "6. Convert Column type to Float.\n",
    "7. drop all values less than $15,000 as that is pretty much the eqivalent of annual pay for minimum wage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Removing the dollar signs\n",
    "jobs_df['salary'] = jobs_df.salary.str.replace('$' , '') \n",
    "\n",
    "# Remove everything after dashes\n",
    "jobs_df['salary'] = jobs_df['salary'].apply(lambda x: x.split('-')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function for converting hourly pay to annual salary\n",
    "def day_to_year(item):\n",
    "    if 'an hour' in item:\n",
    "        item = item.replace('an hour', '')\n",
    "        item = float(item)*2000\n",
    "        return item\n",
    "    else:\n",
    "        return item\n",
    "    \n",
    "# Actually converting all the hourly positions to yearly salary\n",
    "jobs_df['salary'] = jobs_df['salary'].apply(day_to_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#removing all intances of 'a day', 'a month', 'a year' and commas so i can convert to ints\n",
    "jobs_df['salary'] = jobs_df.salary.str.replace('a year' , '')\n",
    "jobs_df['salary'] = jobs_df.salary.str.replace('a month' , '')\n",
    "jobs_df['salary'] = jobs_df.salary.str.replace('a day' , '')\n",
    "jobs_df['salary'] = jobs_df.salary.str.replace(',' , '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting dtype to float\n",
    "jobs_df['salary'] = jobs_df['salary'].convert_objects(convert_numeric=True)\n",
    "\n",
    "# Dropping all values below wage criteria.  \n",
    "jobs_df = jobs_df[jobs_df.salary > 15000]\n",
    "\n",
    "# Check what I am left with\n",
    "jobs_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>company</th>\n",
       "      <th>location</th>\n",
       "      <th>salary</th>\n",
       "      <th>description</th>\n",
       "      <th>url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Senior Machine Learning Data Scientist</td>\n",
       "      <td>All-InAnalytics</td>\n",
       "      <td>Austin,TX</td>\n",
       "      <td>140000</td>\n",
       "      <td>\\nMachine Learning Data Scientist. Proficient ...</td>\n",
       "      <td>http://www.indeed.com/rc/clk?jk=b87633408f9b73...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    title          company   location  salary  \\\n",
       "0  Senior Machine Learning Data Scientist  All-InAnalytics  Austin,TX  140000   \n",
       "\n",
       "                                         description  \\\n",
       "0  \\nMachine Learning Data Scientist. Proficient ...   \n",
       "\n",
       "                                                 url  \n",
       "0  http://www.indeed.com/rc/clk?jk=b87633408f9b73...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just having a peak at the current dataframes structure\n",
    "jobs_df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframe Cleaning: Part II\n",
    "our index is all messy after all the dropped values.\n",
    "1. Reset the index\n",
    "2. Remove the column 'Unnamed: 0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Reseting index and dropping the 'index' made column that results.  \n",
    "jobs_df.reset_index(inplace = True)\n",
    "jobs_df.drop('index', axis =1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-Scraping to get the descriptions of the positions left in my cleaned dataframe.  \n",
    "1. Build sub scraper\n",
    "2. Build vanishing dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This subscraper will only work on Indeed.com pages( or those that have the exact same java layout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "description = []\n",
    "\n",
    "def sub_scraper(url):\n",
    "    submarine = requests.get(url)\n",
    "    if submarine.status_code == 200:\n",
    "        sub_data = submarine.content\n",
    "        sub_soup = BeautifulSoup(sub_data)\n",
    "        post = 'none'\n",
    "        for element in sub_soup.findAll(\"span\",{\"id\":\"job_summary\"}):\n",
    "            post = element.text\n",
    "    else:\n",
    "        post = 'none'\n",
    "    description.append(post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# running the subscraper\n",
    "for link in jobs_df['url']:\n",
    "    sub_scraper(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this dataframe has the position descriptions concatted to it now.  \n",
    "description_df =pd.DataFrame(description)\n",
    "jobs_df_whole = pd.concat([jobs_df, description_df],axis =1)\n",
    "jobs_df_whole = jobs_df_whole.rename(columns = {0:'full_description'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict a binary variable - whether the salary was low or high. \n",
    "\n",
    "Compute the median salary and create a new binary variable that is true when the salary is high (above the median)¶\n",
    "We could also perform Linear Regression (or any regression) to predict the salary value here. Instead, we are going to convert this into a binary classification problem, by predicting two classes, HIGH vs LOW salary.\n",
    "\n",
    "While performing regression may be better, performing classification may help remove some of the noise of the extreme salaries. We don't have to choice the median as the splitting point - we could also split on the 75th percentile or any other reasonable breaking point.\n",
    "\n",
    "In fact, the ideal scenario may be to predict many levels of salaries (which i have prepared for)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count        20.000000\n",
       "mean      80545.050000\n",
       "std       46286.859594\n",
       "min       32900.000000\n",
       "25%       40000.000000\n",
       "50%       62500.000000\n",
       "75%      125000.000000\n",
       "max      160000.000000\n",
       "Name: salary, dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# lets find the quartile ranges to identify high, low, and inbtween positions.  \n",
    "jobs_df_whole['salary'].describe()\n",
    "# 62500 will be out delimiter, the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set my y in my field.  \n",
    "jobs_df_whole['pay_grade'] = np.where(jobs_df_whole['salary']>=62500, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Categories \n",
    "outliers are 1.5*IQR away from IQR.\n",
    "\n",
    "iqr = 125000 - 40000 = 85000.\n",
    "\n",
    "Lower outliers =  32900 - 85000 = -52100. (dont need to worry about this)\n",
    "\n",
    "Upper Outliers = 160000 + 85000 = 245000.\n",
    "\n",
    "Very Low < -52100 (not possible with this dataset)\n",
    "\n",
    "low < 40000\n",
    "\n",
    "medium low < 62500\n",
    "\n",
    "Medium High < 125000\n",
    "\n",
    "High < 245000\n",
    "\n",
    "Very High > 245000 ( also not possible with this sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pay_scaler(row):\n",
    "        if row['salary'] < 40000:\n",
    "            value = 'low'\n",
    "        elif row['salary'] < 62500:\n",
    "            value = 'med_low'\n",
    "        elif row['salary'] < 125000:\n",
    "            value = 'med_high'\n",
    "        elif row['salary'] < 245000:\n",
    "            value = 'high'\n",
    "        else:\n",
    "            value = 'very_high'\n",
    "        return value\n",
    "    \n",
    "jobs_df_whole['pay_scale']= jobs_df_whole.apply(pay_scaler, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we got the complete data that we can work with, we may want to save it incase anything happens. \n",
    "jobs_df_whole.to_csv('indeed_jobs_ideal.csv',encoding='utf-8')\n",
    "jobs_df_whole = pd.read_csv('indeed_jobs_ideal.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Need to create a sub dataset for all the rows i have that dont have a null value in the full description row.\n",
    "jobs_df_pure = jobs_df_whole[~jobs_df_whole.full_description.str.contains(\"none\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 9)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the shape of whats left\n",
    "jobs_df_pure.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty tiny, I only scraped 100 results to begin with for times sake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to predict a binary variable - whether the salary was low or high. Compute the median salary and create a new binary variable that is true when the salary is high (above the median)\n",
    "We could also perform Linear Regression (or any regression) to predict the salary value here. Instead, we are going to convert this into a binary classification problem, by predicting two classes, HIGH vs LOW salary.\n",
    "While performing regression may be better, performing classification may help remove some of the noise of the extreme salaries. We don't have to choice the median as the splitting point - we could also split on the 75th percentile or any other reasonable breaking point.\n",
    "In fact, the ideal scenario may be to predict many levels of salaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Separate Necessary Features into x and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = jobs_df_pure[['full_description']]\n",
    "# for binary\n",
    "y1 = jobs_df_pure['pay_grade']\n",
    "# for multi-class\n",
    "y2 = jobs_df_pure['pay_scale']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Apply TFIDF Vectorizer.\n",
    "Term Frequency Inverse Document Frequency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "X_TFIDF = tvec.fit_transform(X['full_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "# set 1 is for the boolean y (pay_grade)\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_TFIDF, y1, test_size = 0.25, random_state = 21)\n",
    "\n",
    "# set 2 is for the categorical y (pay_scale)\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_TFIDF, y2, test_size = 0.25, random_state = 21)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Random Forest\n",
    "Binary and multi-class classification\n",
    "\n",
    "Create a Random Forest model to predict High/Low salary using statsmodel. Start by ONLY using the location as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This can be put in the cross validation parameters.  \n",
    "cv1 = StratifiedKFold(y1, n_folds=3, shuffle=True, random_state=41)\n",
    "cv2 = StratifiedKFold(y2, n_folds=3, shuffle=True, random_state=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree for Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Score:\t0.667 ± 0.236\n"
     ]
    }
   ],
   "source": [
    "dt1 = DecisionTreeClassifier(class_weight='balanced')\n",
    "c_val_score1 = cross_val_score(dt1, X_TFIDF, y1, cv=cv1, n_jobs=-1)\n",
    "# using original values as CV automatically does a train-test split\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Decision Tree\", c_val_score1.mean().round(3), c_val_score1.std().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree for Multi-Class Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Score:\t0.111 ± 0.157\n"
     ]
    }
   ],
   "source": [
    "dt2 = DecisionTreeClassifier(class_weight='balanced')\n",
    "c_val_score2 = cross_val_score(dt2, X_TFIDF, y2, cv=cv2, n_jobs=-1)\n",
    "# using original values as CV automatically does a train-test split\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Decision Tree\", c_val_score2.mean().round(3), c_val_score2.std().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.040000000000000036"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfr = RandomForestRegressor()\n",
    "rfr.fit(X_train1, y_train1)\n",
    "y_pred1 = rfr.predict(X_test1)\n",
    "rfr.score(X_test1, y_test1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "rfc.fit(X_train1, y_train1)\n",
    "y_pred1 = rfc.predict(X_test1)\n",
    "# convert x test to an array before scoring. np.to array opr ravel\n",
    "X_test1_arr = X_test1.toarray()\n",
    "rfc.score(X_test1_arr, y_test1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier for Multi-Class Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "rfc.fit(X_train2, y_train2)\n",
    "\n",
    "y_pred2 = rfc.predict(X_test2)\n",
    "\n",
    "X_test2_arr = X_test2.toarray()\n",
    "\n",
    "rfc.score(X_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier()\n",
    "\n",
    "cvs2 = cross_val_score(rfc, X_train2, y_train2, cv = 12)\n",
    "\n",
    "cvs2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Logistic Regression Model\n",
    "Binary Classification only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "\n",
    "X = X_TFIDF # Term Frequency Inverser Document Frequency\n",
    "y1 = jobs_df_pure['pay_grade']\n",
    "\n",
    "X_train_lr, X_test_lr, y_train_lr, y_test_lr = train_test_split(X_TFIDF, y1, test_size = 0.25, random_state = 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.fit(X_train_lr, y_train_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred_lr = lr.predict(X_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_test_lr,y_test_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
