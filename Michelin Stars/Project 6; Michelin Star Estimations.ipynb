{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "\n",
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.metrics import f1_score, log_loss, precision_score, recall_score \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aquisition/Wrangling/Scraping\n",
    "Scraping michelin restaurant reviews for only the United States, which only exist for San Francisco, New-York and Chicago.  I elected to only scrape the information for results within the United States for maintaining cultural similarities.  \n",
    "\n",
    "My Test data will be scraped from the Washintonian top 100 Resturants for 2016.  The structure by which the reviews are written is very similar to actual Michelin reviews.  Additionally I don't have to scrap every restaurant in DC and eliminate things like chain restaurants and navigate around biased unprofessional reviews.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For both Michelin's website and The Washingtonian I had to scrape in two parts.  First, get the url's for the actual review pages. Second,  run those urls in a different scraper to get the desired data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Baseline/test scraper\n",
    "def michelin_scraper(url):\n",
    "    page = requests.get(url)\n",
    "    page_data = page.content\n",
    "    page_soup = BeautifulSoup(page_data, \"lxml\")\n",
    "    \n",
    "    \n",
    "    for item in page_soup.findAll('li',{'class':'poi-item poi-item-restaurant'}):\n",
    "    \n",
    "        direction = item.find(\"a\").get(\"href\")\n",
    "        print 'https://www.viamichelin.com'+direction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# scrapers for individual cities.\n",
    "\n",
    "# New York, New York\n",
    "NY_URL = []\n",
    "def michelin_scraper_ny(url):\n",
    "    page = requests.get(url)\n",
    "    page_data = page.content\n",
    "    page_soup = BeautifulSoup(page_data, \"lxml\")\n",
    "    \n",
    "    \n",
    "    for item in page_soup.findAll('li',{'class':'poi-item poi-item-restaurant'}):\n",
    "    \n",
    "        direction = item.find(\"a\").get(\"href\")\n",
    "        NY_URL.append('https://www.viamichelin.com'+direction)\n",
    "\n",
    "# Chicago, Illinois\n",
    "CHI_URL = []\n",
    "def michelin_scraper_chi(url):\n",
    "    page = requests.get(url)\n",
    "    page_data = page.content\n",
    "    page_soup = BeautifulSoup(page_data, \"lxml\")\n",
    "    \n",
    "    \n",
    "    for item in page_soup.findAll('li',{'class':'poi-item poi-item-restaurant'}):\n",
    "    \n",
    "        direction = item.find(\"a\").get(\"href\")\n",
    "        CHI_URL.append('https://www.viamichelin.com'+direction)\n",
    "\n",
    "# San Fransisco, California        \n",
    "SF_URL = []        \n",
    "def michelin_scraper_sf(url):\n",
    "    page = requests.get(url)\n",
    "    page_data = page.content\n",
    "    page_soup = BeautifulSoup(page_data, \"lxml\")\n",
    "    \n",
    "    \n",
    "    for item in page_soup.findAll('li',{'class':'poi-item poi-item-restaurant'}):\n",
    "    \n",
    "        direction = item.find(\"a\").get(\"href\")\n",
    "        SF_URL.append('https://www.viamichelin.com'+direction)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ----------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scraping New York's Michelin URLs\n",
    "# I know there are 36 pages of Michelin rated restuarants in New York, thats why I stop there.\n",
    "for i in range(1,37):\n",
    "\n",
    "    link = 'https://www.viamichelin.com/web/Restaurants/Restaurants-New_York-_-New_York-United_States?strLocid=31NG9zOXAxMGNOREF1TnpFek1EVT1jTFRjMExqQXdOekl6&page='+str(i)\n",
    "    michelin_scraper_ny(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scraping Chicago's Michelin URLs\n",
    "# Theres only 13 pages for Chicago\n",
    "for i in range(1,14):\n",
    "    link = 'https://www.viamichelin.com/web/Restaurants/Restaurants-Chicago-_-Illinois-United_States?strLocid=31NG9zYWgxMGNOREV1T0Rnek1qTT1jTFRnM0xqWXpNalE9&page='+str(i)\n",
    "    michelin_scraper_chi(link) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(1,13):\n",
    "# 13 pages for SF as well\n",
    "    link = 'https://www.viamichelin.com/web/Restaurants/Restaurants-San_Francisco-_-California-United_States?strLocid=31NG9zOHAxMGNNemN1Tnpnd01EZz1nTFRFeU1pNDBNakF4Tnc9PQ==&page='+str(i)\n",
    "    michelin_scraper_sf(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create a few dataframes for all the information\n",
    "cols = ['restaurant','type','price','rating', 'review', 'url']\n",
    "NY_DF = pd.DataFrame(columns = cols)\n",
    "CHI_DF = pd.DataFrame(columns = cols)\n",
    "SF_DF = pd.DataFrame(columns = cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling all the Data for Michelin New York restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Building my scraper for testing\n",
    "def ms_page_scraper_ny(link):\n",
    "    page= requests.get(link)\n",
    "    page_data = page.content\n",
    "    page_soup = BeautifulSoup(page_data, \"lxml\")\n",
    "    \n",
    "   \n",
    "    for element in page_soup.findAll('div',{'class':'view-container'}):\n",
    "        # Getting Restaurant name\n",
    "        name =  element.find('div',{'class': 'datasheet-item datasheet-name'}).text\n",
    "        name = name.strip()\n",
    "    \n",
    "        # Cuisine Type\n",
    "        cuisine =  element.find('div',{'class':'datasheet-cooking-type' }).text\n",
    "    \n",
    "        # From Price\n",
    "        price = element.find('div',{'class':'datasheet-price'}).text\n",
    "        price = price.strip()\n",
    "        price =  re.sub('[\\s+]', ' ', price)\n",
    "        #to and from prices will need to be separated, or averaged later.\n",
    "        \n",
    "        # Guide Review\n",
    "        review = element.find('blockquote').text\n",
    "        review = review.strip()\n",
    "        \n",
    "        # Stars (text)\n",
    "        rating = element.find('div',{'class':'datasheet-quotation'}).text\n",
    "        rating = rating.strip()\n",
    "        \n",
    "\n",
    "    NY_DF.loc[len(NY_DF)] = [name, cuisine, price, rating, review, link]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# running the scraper on all the URLs, I got earlier\n",
    "for item in NY_URL:\n",
    "    ms_page_scraper_ny(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving new dataframe as a csv incase kernal gets lost.  \n",
    "NY_DF.to_csv('michelin_NY.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling all the Data for Michelin Chicago restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining scraping function\n",
    "def ms_page_scraper_chi(link):\n",
    "    page= requests.get(link)\n",
    "    page_data = page.content\n",
    "    page_soup = BeautifulSoup(page_data, \"lxml\")\n",
    "    \n",
    "   \n",
    "    for element in page_soup.findAll('div',{'class':'view-container'}):\n",
    "        # Getting Restaurant name\n",
    "        name =  element.find('div',{'class': 'datasheet-item datasheet-name'}).text\n",
    "        name = name.strip()\n",
    "    \n",
    "        # Cuisine Type\n",
    "        cuisine =  element.find('div',{'class':'datasheet-cooking-type' }).text\n",
    "    \n",
    "        # From Price\n",
    "        price = element.find('div',{'class':'datasheet-price'}).text\n",
    "        price = price.strip()\n",
    "        price =  re.sub('[\\s+]', ' ', price)\n",
    "        #to and from prices will need to be separated, or averaged later.\n",
    "        \n",
    "        # Guide Review\n",
    "        review = element.find('blockquote').text\n",
    "        review = review.strip()\n",
    "        \n",
    "        # Stars (text)\n",
    "        rating = element.find('div',{'class':'datasheet-quotation'}).text\n",
    "        rating = rating.strip()\n",
    "        \n",
    "\n",
    "    CHI_DF.loc[len(CHI_DF)] = [name, cuisine, price, rating, review, link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Running Scraping function\n",
    "for item in CHI_URL:\n",
    "    ms_page_scraper_chi(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving results\n",
    "CHI_DF.to_csv('michelin_CHI.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pulling all the Data for Michelin San Francisco restaurants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Defining scraping function\n",
    "def ms_page_scraper_sf(link):\n",
    "    page= requests.get(link)\n",
    "    page_data = page.content\n",
    "    page_soup = BeautifulSoup(page_data, \"lxml\")\n",
    "    \n",
    "   \n",
    "    for element in page_soup.findAll('div',{'class':'view-container'}):\n",
    "        # Getting Restaurant name\n",
    "        name =  element.find('div',{'class': 'datasheet-item datasheet-name'}).text\n",
    "        name = name.strip()\n",
    "    \n",
    "        # Cuisine Type\n",
    "        cuisine =  element.find('div',{'class':'datasheet-cooking-type' }).text\n",
    "    \n",
    "        # From Price\n",
    "        price = element.find('div',{'class':'datasheet-price'}).text\n",
    "        price = price.strip()\n",
    "        price =  re.sub('[\\s+]', ' ', price)\n",
    "        #to and from prices will need to be separated, or averaged later.\n",
    "        \n",
    "        # Guide Review\n",
    "        review = element.find('blockquote').text\n",
    "        review = review.strip()\n",
    "        \n",
    "        # Stars (text)\n",
    "        rating = element.find('div',{'class':'datasheet-quotation'}).text\n",
    "        rating = rating.strip()\n",
    "        \n",
    "\n",
    "    SF_DF.loc[len(SF_DF)] = [name, cuisine, price, rating, review, link]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running Scraping function\n",
    "for item in SF_URL:\n",
    "    ms_page_scraper_sf(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving results\n",
    "SF_DF.to_csv('michelin_SF.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location where Test Data is being pulled from.\n",
    "https://www.washingtonian.com/2016/02/08/100-very-best-restaurants/2016/\n",
    "- this is a two part scrap where first I have to get the links to the individual pages and then scrape the individual web pages.  While this can be done in a single function, I choose to keep it separate for simplicity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining scraper (Part. 1) function for Washintonian\n",
    "washington_100 = []\n",
    "\n",
    "def washintonian_scraper(link):\n",
    "    page = requests.get(link)\n",
    "    page_data = page.content\n",
    "    page_soup = BeautifulSoup(page_data, \"lxml\")\n",
    "    \n",
    "    links = page_soup.find_all('tr')\n",
    "    for tag in links:\n",
    "        link = tag.get('data-href',None)\n",
    "        washington_100.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Running scraper Part. 1)\n",
    "washintonian_scraper('https://www.washingtonian.com/2016/02/08/100-very-best-restaurants/2016/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Defining scraper (Part. 2)\n",
    "def washingtonian_page_scraper(link):\n",
    "    page_r = requests.get(link)\n",
    "    page_r_data = page_r.content\n",
    "    page_r_soup = BeautifulSoup(page_r_data, \"lxml\")\n",
    "    restaurant_page = page_r_soup.find_all('div',{'id': 'content'})\n",
    "    for obj in restaurant_page:\n",
    "        \n",
    "        name = obj.find(\"div\",{'class':'section'}).text\n",
    "        \n",
    "        cuisine = obj.find('div',{'class':'type'}).text\n",
    "        cuisine =cuisine.strip()\n",
    "        \n",
    "        price = obj.find('div',{'class':'price'}).text\n",
    "        price = price.strip()\n",
    "        \n",
    "        review = obj.find('p').text\n",
    "        \n",
    "        8\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing washingtonian 100 dataframe\n",
    "column = ['restaurant','type','price','review','url']\n",
    "wash_100_df  = pd.DataFrame(columns = column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# there was a None value in my list that needed to be removed.\n",
    "washington_100 = filter(None, washington_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for item in washington_100:\n",
    "    washingtonian_page_scraper(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Saving to CSV end results.\n",
    "NY_DF  = pd.read_csv('data/michelin_NY.csv')\n",
    "CHI_DF = pd.read_csv('data/michelin_CHI.csv')\n",
    "SF_DF = pd.read_csv('data/michelin_SF.csv')\n",
    "washington_100 = pd.read_csv('data/wash_100.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning, Joining and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merging Train Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Concating all the city DFs into one DF\n",
    "MICH_US_DF = pd.concat([NY_DF,CHI_DF,SF_DF])\n",
    "\n",
    "#Dropping extra column that came from reading the csvs back in.\n",
    "MICH_US_DF.drop('Unnamed: 0', axis =1, inplace =True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting Price Column into a numeric value from a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Converting Price column to numeric values.\n",
    "#MICH_US_DF['price'].value_counts()\n",
    "\n",
    "Price = []\n",
    "\n",
    "for row in MICH_US_DF['price']:\n",
    "    if row == 'From 13 USD    to 24 USD':\n",
    "        Price.append(1)  \n",
    "\n",
    "    elif row == 'From 25 USD    to 49 USD':\n",
    "        Price.append(2)\n",
    "\n",
    "    elif row == 'From 50 USD    to 74 USD':\n",
    "        Price.append(3)\n",
    "\n",
    "    elif row == 'From 75 USD    to 150 USD':\n",
    "        Price.append(4)\n",
    "\n",
    "# There are only 4 values that appear in this column\n",
    "# 'From 13 USD    to 24 USD' : 1\n",
    "# 'From 25 USD    to 49 USD' : 2\n",
    "# 'From 50 USD    to 74 USD' : 3\n",
    "# 'From 75 USD    to 150 USD': 4\n",
    "\n",
    "MICH_US_DF['Price'] = Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting Rating to a numeric Value from Strings\n",
    "While there are considerably more categories in the this feature I am only intersted in distinguishing stars from no stars and counting the number of starts.  Michelin has ratings like \"Good Standard\" and \"Bib Gourmand\" and these will be our false and 0 values.  Im creating both a categorical Y and Boolean Y train sets.\n",
    "\n",
    "Rating : How many starts (if any) did that restaurant recieve. This will be used for classification models.\n",
    "\n",
    "Star : Boolean as to whether the restaurant did recieve a star(s) or not.  This will be useful for logistic regressions and  SVMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MICH_US_DF['rating'].value_counts()\n",
    "\n",
    "rates = []\n",
    "star = []\n",
    "\n",
    "for rate in MICH_US_DF['rating']:\n",
    "    if 'A MICHELIN star' in str(rate):\n",
    "        rates.append(1)\n",
    "        star.append(1)\n",
    "        \n",
    "    elif 'Two MICHELIN stars' in str(rate):\n",
    "        rates.append(2)\n",
    "        star.append(1)\n",
    "        \n",
    "    elif 'Three MICHELIN stars' in str(rate):\n",
    "        rates.append(3)\n",
    "        star.append(1)\n",
    "        \n",
    "    else:\n",
    "        rates.append(0)\n",
    "        star.append(0)\n",
    "        \n",
    "MICH_US_DF['Rating'] = rates\n",
    "MICH_US_DF['Star'] = star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Indexes from initial dataframes were kept and thus needed to be reset.\n",
    "MICH_US_DF.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stripping out the word 'Cuisine' from the type column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type_list = MICH_US_DF['type'].tolist()\n",
    "\n",
    "new_list = []\n",
    "for item in type_list:\n",
    "    new = item.replace('Cuisine ', '')\n",
    "    new_list.append(new)\n",
    "    \n",
    "MICH_US_DF['Type'] = new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# dropping a column that appeared (I believe this occured when I imported back from a csv)\n",
    "washington_100.drop('Unnamed: 0', axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Converting a Price stated in dollar signs to a numeric value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# washington_100['price'].value_counts()\n",
    "\n",
    "price = []\n",
    "\n",
    "for dollars in washington_100['price']:\n",
    "    if dollars == '$':\n",
    "        price.append(1)\n",
    "    elif dollars == '$$':\n",
    "        price.append(2)\n",
    "    elif dollars == '$$$':\n",
    "        price.append(3)\n",
    "    elif dollars == '$$$$':\n",
    "        price.append(4)\n",
    "        \n",
    "washington_100['Price'] = price       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Going to have to remove the second adjective from the type column\n",
    "Some of the 'Type' categories had two or even three descriptive values.  It will make things much easier to eliminate the secondary types.\n",
    "\n",
    "Additionally, this can be useful in a dummies varable as a restaurant can easily have more than one category while using dummies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "wash_type_list = washington_100['type'].tolist()\n",
    "\n",
    "wash_type_new = []\n",
    "for item in wash_type_list:\n",
    "\n",
    "    sep = ','\n",
    "    new = item.split(sep)[0]\n",
    "    wash_type_new.append(new)\n",
    "    \n",
    "washington_100['Type'] = wash_type_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Dataframes that will be used for the modeling and predictcting (Data Refinement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MICH = MICH_US_DF[['restaurant', 'Type','Price','review']]\n",
    "WASH = washington_100[['restaurant','Type','Price','review']]\n",
    "\n",
    "MICH2 = MICH_US_DF[['restaurant', 'Type','Price','review', 'Rating', 'Star']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>restaurant</th>\n",
       "      <th>Type</th>\n",
       "      <th>Price</th>\n",
       "      <th>review</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Racines NY</td>\n",
       "      <td>French</td>\n",
       "      <td>4</td>\n",
       "      <td>The American outpost of this popular Parisian ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Little Park</td>\n",
       "      <td>American</td>\n",
       "      <td>2</td>\n",
       "      <td>Chef/owner Andrew Carmellini strikes again, th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Blaue Gans</td>\n",
       "      <td>Austrian</td>\n",
       "      <td>2</td>\n",
       "      <td>This sleek, unbridled Viennese-style café feel...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Rosanjin</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>4</td>\n",
       "      <td>From its ultra-discrete entrance to its heavil...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Brushstroke</td>\n",
       "      <td>Japanese</td>\n",
       "      <td>4</td>\n",
       "      <td>The name may not give too much away but as soo...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    restaurant      Type  Price  \\\n",
       "0   Racines NY    French      4   \n",
       "1  Little Park  American      2   \n",
       "2   Blaue Gans  Austrian      2   \n",
       "3     Rosanjin  Japanese      4   \n",
       "4  Brushstroke  Japanese      4   \n",
       "\n",
       "                                              review  Rating  Star  \n",
       "0  The American outpost of this popular Parisian ...       0     0  \n",
       "1  Chef/owner Andrew Carmellini strikes again, th...       0     0  \n",
       "2  This sleek, unbridled Viennese-style café feel...       0     0  \n",
       "3  From its ultra-discrete entrance to its heavil...       1     1  \n",
       "4  The name may not give too much away but as soo...       1     1  "
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MICH2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MICH2.to_csv('mich_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing \n",
    "In order to vectorize the review data with the same number of features so that the test data is compatible with the train model I will need to TFIDF them simultaneously.\n",
    "The Michelin reviews (MICH) will be added to the bottom of the washington reviews (WASH) as 100 is a good cut off point that is easy to remember when splitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adding train and test sets together.\n",
    "ALL = pd.concat([WASH[['Type','review']], MICH[['Type', 'review']]])\n",
    "\n",
    "# while the gangs all here, lets conver the 'Type' feature into dummie values.\n",
    "dummies = pd.get_dummies(ALL['Type'], drop_first=True)\n",
    "\n",
    "# Put dummies back in ALL\n",
    "ALL = pd.concat([ALL, dummies], axis = 1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummies seems to have created properly with no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Once again indexes have to be reset as ALL has indexes '0'-'99' twice.\n",
    "ALL.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TFIDF - Term Frequency Inverse Document Frequency\n",
    "-  Takes all the words in our target feature ('review') and calculates the relation of each individual word to that specific cell (review) as well as the entire corpus (all reviews).  With this we can find the predictive power of each word in comparison with that particular review and the entire corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Term Frequency Inverse Document Frequency-ing the data\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tvec = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "review_TFIDF = tvec.fit_transform(ALL['review'])\n",
    "\n",
    "x = pd.DataFrame(review_TFIDF.toarray())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ALL_TFIDF = pd.concat([ALL, x], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ALL_TFIDF.drop(['Type', 'review'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Splitting the data to original train(MICH) and test(WASH)\n",
    "mich_tfidf = ALL_TFIDF.loc[100: ,]\n",
    "wash_tfidf = ALL_TFIDF.loc[0:99 ,]\n",
    "\n",
    "# reseting index so it can be merged back with Rating and Star\n",
    "mich_tfidf.reset_index(drop = True, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I Have the Michelin and Washington Data that has been TFIDF'd as well as Dummie variabled.  Price is not included in these dataframes so i will need to add it.  I think I am going to make the restaurant the index so I know what rows are what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merging the categorical turned numerical features back to the descriptive features.  \n",
    "mich_all = pd.concat([MICH2, mich_tfidf], axis = 1 )\n",
    "wash_all = pd.concat([WASH, wash_tfidf], axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mich_all.set_index('restaurant', inplace = True)\n",
    "wash_all.set_index('restaurant', inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## X Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Removing features that will not be used in the model ()\n",
    "WASH_X = wash_all.drop(['review', 'Type',], axis = 1)\n",
    "\n",
    "MICH_X = mich_all.drop(['review', 'Type', 'Rating','Star'], axis =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Y Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Making the Y Value(s)\n",
    "MICH_Y = mich_all['Star']\n",
    "\n",
    "MICH_Y2 = mich_all['Rating']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MICH_X.to_csv('MICH_X.csv', encoding = 'utf-8')\n",
    "MICH_Y.to_csv('MICH_Y.csv', encoding = 'utf-8')\n",
    "MICH_Y2.to_csv('MICH_Y2.csv', encoding = 'utf-8')\n",
    "WASH_X.to_csv('WASH_X.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA: Logistic Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.946933962264\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[770,  11],\n",
       "       [ 34,  33]])"
      ]
     },
     "execution_count": 545,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(MICH_X, MICH_Y, test_size = 0.5)\n",
    "\n",
    "lr = LogisticRegression(penalty = 'l1')\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "pred_lr = lr.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, pred_lr)\n",
    "\n",
    "confusion_matrix(y_test, pred_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>American</th>\n",
       "      <th>Argentinian</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Austrian</th>\n",
       "      <th>Barbecue</th>\n",
       "      <th>Basque</th>\n",
       "      <th>Belgian</th>\n",
       "      <th>Brazilian</th>\n",
       "      <th>Burmese</th>\n",
       "      <th>...</th>\n",
       "      <th>15873</th>\n",
       "      <th>15874</th>\n",
       "      <th>15875</th>\n",
       "      <th>15876</th>\n",
       "      <th>15877</th>\n",
       "      <th>15878</th>\n",
       "      <th>15879</th>\n",
       "      <th>15880</th>\n",
       "      <th>15881</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restaurant</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Komi</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Plume</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Inn at Little Washington</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Source</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blue Duck Tavern</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bistro Bis</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Marcel's</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minibar</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Del Campo</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Ashby Inn &amp; Restaurant</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Restaurant Eve</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Restaurant at Patowmack Farm</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equinox</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>L’Auberge Chez François and Jacques’ Brasserie</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14 rows × 15953 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Price  American  Argentinian  \\\n",
       "restaurant                                                                     \n",
       "Komi                                              4.0       0.0          0.0   \n",
       "Plume                                             4.0       1.0          0.0   \n",
       "The Inn at Little Washington                      4.0       1.0          0.0   \n",
       "The Source                                        4.0       0.0          0.0   \n",
       "Blue Duck Tavern                                  4.0       1.0          0.0   \n",
       "Bistro Bis                                        4.0       0.0          0.0   \n",
       "Marcel's                                          4.0       0.0          0.0   \n",
       "Minibar                                           4.0       1.0          0.0   \n",
       "Del Campo                                         4.0       0.0          0.0   \n",
       "The Ashby Inn & Restaurant                        4.0       1.0          0.0   \n",
       "Restaurant Eve                                    4.0       1.0          0.0   \n",
       "The Restaurant at Patowmack Farm                  4.0       1.0          0.0   \n",
       "Equinox                                           4.0       1.0          0.0   \n",
       "L’Auberge Chez François and Jacques’ Brasserie    4.0       0.0          0.0   \n",
       "\n",
       "                                                Asian  Austrian  Barbecue  \\\n",
       "restaurant                                                                  \n",
       "Komi                                              0.0       0.0       0.0   \n",
       "Plume                                             0.0       0.0       0.0   \n",
       "The Inn at Little Washington                      0.0       0.0       0.0   \n",
       "The Source                                        0.0       0.0       0.0   \n",
       "Blue Duck Tavern                                  0.0       0.0       0.0   \n",
       "Bistro Bis                                        0.0       0.0       0.0   \n",
       "Marcel's                                          0.0       0.0       0.0   \n",
       "Minibar                                           0.0       0.0       0.0   \n",
       "Del Campo                                         0.0       0.0       0.0   \n",
       "The Ashby Inn & Restaurant                        0.0       0.0       0.0   \n",
       "Restaurant Eve                                    0.0       0.0       0.0   \n",
       "The Restaurant at Patowmack Farm                  0.0       0.0       0.0   \n",
       "Equinox                                           0.0       0.0       0.0   \n",
       "L’Auberge Chez François and Jacques’ Brasserie    0.0       0.0       0.0   \n",
       "\n",
       "                                                Basque  Belgian  Brazilian  \\\n",
       "restaurant                                                                   \n",
       "Komi                                               0.0      0.0        0.0   \n",
       "Plume                                              0.0      0.0        0.0   \n",
       "The Inn at Little Washington                       0.0      0.0        0.0   \n",
       "The Source                                         0.0      0.0        0.0   \n",
       "Blue Duck Tavern                                   0.0      0.0        0.0   \n",
       "Bistro Bis                                         0.0      0.0        0.0   \n",
       "Marcel's                                           0.0      0.0        0.0   \n",
       "Minibar                                            0.0      0.0        0.0   \n",
       "Del Campo                                          0.0      0.0        0.0   \n",
       "The Ashby Inn & Restaurant                         0.0      0.0        0.0   \n",
       "Restaurant Eve                                     0.0      0.0        0.0   \n",
       "The Restaurant at Patowmack Farm                   0.0      0.0        0.0   \n",
       "Equinox                                            0.0      0.0        0.0   \n",
       "L’Auberge Chez François and Jacques’ Brasserie     0.0      0.0        0.0   \n",
       "\n",
       "                                                Burmese  ...   15873  15874  \\\n",
       "restaurant                                               ...                  \n",
       "Komi                                                0.0  ...     0.0    0.0   \n",
       "Plume                                               0.0  ...     0.0    0.0   \n",
       "The Inn at Little Washington                        0.0  ...     0.0    0.0   \n",
       "The Source                                          0.0  ...     0.0    0.0   \n",
       "Blue Duck Tavern                                    0.0  ...     0.0    0.0   \n",
       "Bistro Bis                                          0.0  ...     0.0    0.0   \n",
       "Marcel's                                            0.0  ...     0.0    0.0   \n",
       "Minibar                                             0.0  ...     0.0    0.0   \n",
       "Del Campo                                           0.0  ...     0.0    0.0   \n",
       "The Ashby Inn & Restaurant                          0.0  ...     0.0    0.0   \n",
       "Restaurant Eve                                      0.0  ...     0.0    0.0   \n",
       "The Restaurant at Patowmack Farm                    0.0  ...     0.0    0.0   \n",
       "Equinox                                             0.0  ...     0.0    0.0   \n",
       "L’Auberge Chez François and Jacques’ Brasserie      0.0  ...     0.0    0.0   \n",
       "\n",
       "                                                15875  15876  15877  15878  \\\n",
       "restaurant                                                                   \n",
       "Komi                                              0.0    0.0    0.0    0.0   \n",
       "Plume                                             0.0    0.0    0.0    0.0   \n",
       "The Inn at Little Washington                      0.0    0.0    0.0    0.0   \n",
       "The Source                                        0.0    0.0    0.0    0.0   \n",
       "Blue Duck Tavern                                  0.0    0.0    0.0    0.0   \n",
       "Bistro Bis                                        0.0    0.0    0.0    0.0   \n",
       "Marcel's                                          0.0    0.0    0.0    0.0   \n",
       "Minibar                                           0.0    0.0    0.0    0.0   \n",
       "Del Campo                                         0.0    0.0    0.0    0.0   \n",
       "The Ashby Inn & Restaurant                        0.0    0.0    0.0    0.0   \n",
       "Restaurant Eve                                    0.0    0.0    0.0    0.0   \n",
       "The Restaurant at Patowmack Farm                  0.0    0.0    0.0    0.0   \n",
       "Equinox                                           0.0    0.0    0.0    0.0   \n",
       "L’Auberge Chez François and Jacques’ Brasserie    0.0    0.0    0.0    0.0   \n",
       "\n",
       "                                                15879  15880  15881  pred  \n",
       "restaurant                                                                 \n",
       "Komi                                              0.0    0.0    0.0   1.0  \n",
       "Plume                                             0.0    0.0    0.0   1.0  \n",
       "The Inn at Little Washington                      0.0    0.0    0.0   1.0  \n",
       "The Source                                        0.0    0.0    0.0   1.0  \n",
       "Blue Duck Tavern                                  0.0    0.0    0.0   1.0  \n",
       "Bistro Bis                                        0.0    0.0    0.0   1.0  \n",
       "Marcel's                                          0.0    0.0    0.0   1.0  \n",
       "Minibar                                           0.0    0.0    0.0   1.0  \n",
       "Del Campo                                         0.0    0.0    0.0   1.0  \n",
       "The Ashby Inn & Restaurant                        0.0    0.0    0.0   1.0  \n",
       "Restaurant Eve                                    0.0    0.0    0.0   1.0  \n",
       "The Restaurant at Patowmack Farm                  0.0    0.0    0.0   1.0  \n",
       "Equinox                                           0.0    0.0    0.0   1.0  \n",
       "L’Auberge Chez François and Jacques’ Brasserie    0.0    0.0    0.0   1.0  \n",
       "\n",
       "[14 rows x 15953 columns]"
      ]
     },
     "execution_count": 554,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wash_pred = lr.predict(WASH_X)\n",
    "WASH_X_LR_PRED = WASH_X\n",
    "WASH_X_LR_PRED['pred'] = wash_pred\n",
    "WASH_LR_PRED = WASH_X_LR_PRED.where(WASH_X_LR_PRED['pred'] == 1)\n",
    "WASH_LR_PRED.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### None from the Honerable Mentions list made it in, so theres that.  Looks like 'Price is a highly distinguishing factor.  I double checked and there are 19 restaurants with price of 4 and there are only 14 in this prediction so it is not entirely based upon price.\n",
    "- Bad Saint\n",
    "- Bidwell\n",
    "- Boqueria\n",
    "- Chercher\n",
    "- China Chilcano\n",
    "- Das\n",
    "- Doi Moi\n",
    "- Jaleo\n",
    "- Kyirisan\n",
    "- Lapis\n",
    "- Maketto\n",
    "- Ottoman Taverna\n",
    "- Oyamel\n",
    "- Pearl Dive Oyster Palace\n",
    "- Red Hen\n",
    "- Royal\n",
    "- Thip Khao\n",
    "- 2Amys\n",
    "- Zaytinya"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression w/ multiple categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.920990566038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[762,  15,   0,   0],\n",
       "       [ 36,  19,   0,   0],\n",
       "       [  2,   8,   0,   0],\n",
       "       [  0,   6,   0,   0]])"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(MICH_X, MICH_Y2, test_size = 0.5)\n",
    "lr2 = LogisticRegression(penalty = 'l1')\n",
    "lr2.fit(X_train, y_train)\n",
    "pred_lr2 = lr2.predict(X_test)\n",
    "print accuracy_score(y_test, pred_lr2)\n",
    "confusion_matrix(y_test, pred_lr2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WASH_X.drop('pred', axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Price</th>\n",
       "      <th>American</th>\n",
       "      <th>Argentinian</th>\n",
       "      <th>Asian</th>\n",
       "      <th>Austrian</th>\n",
       "      <th>Barbecue</th>\n",
       "      <th>Basque</th>\n",
       "      <th>Belgian</th>\n",
       "      <th>Brazilian</th>\n",
       "      <th>Burmese</th>\n",
       "      <th>...</th>\n",
       "      <th>15873</th>\n",
       "      <th>15874</th>\n",
       "      <th>15875</th>\n",
       "      <th>15876</th>\n",
       "      <th>15877</th>\n",
       "      <th>15878</th>\n",
       "      <th>15879</th>\n",
       "      <th>15880</th>\n",
       "      <th>15881</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restaurant</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Komi</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Little Serow</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Plume</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Inn at Little Washington</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Source</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Blue Duck Tavern</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Minibar</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Del Campo</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Ashby Inn &amp; Restaurant</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Restaurant Eve</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>The Restaurant at Patowmack Farm</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Equinox</th>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 15953 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Price  American  Argentinian  Asian  \\\n",
       "restaurant                                                              \n",
       "Komi                                4.0       0.0          0.0    0.0   \n",
       "Little Serow                        3.0       0.0          0.0    0.0   \n",
       "Plume                               4.0       1.0          0.0    0.0   \n",
       "The Inn at Little Washington        4.0       1.0          0.0    0.0   \n",
       "The Source                          4.0       0.0          0.0    0.0   \n",
       "Blue Duck Tavern                    4.0       1.0          0.0    0.0   \n",
       "Minibar                             4.0       1.0          0.0    0.0   \n",
       "Del Campo                           4.0       0.0          0.0    0.0   \n",
       "The Ashby Inn & Restaurant          4.0       1.0          0.0    0.0   \n",
       "Restaurant Eve                      4.0       1.0          0.0    0.0   \n",
       "The Restaurant at Patowmack Farm    4.0       1.0          0.0    0.0   \n",
       "Equinox                             4.0       1.0          0.0    0.0   \n",
       "\n",
       "                                  Austrian  Barbecue  Basque  Belgian  \\\n",
       "restaurant                                                              \n",
       "Komi                                   0.0       0.0     0.0      0.0   \n",
       "Little Serow                           0.0       0.0     0.0      0.0   \n",
       "Plume                                  0.0       0.0     0.0      0.0   \n",
       "The Inn at Little Washington           0.0       0.0     0.0      0.0   \n",
       "The Source                             0.0       0.0     0.0      0.0   \n",
       "Blue Duck Tavern                       0.0       0.0     0.0      0.0   \n",
       "Minibar                                0.0       0.0     0.0      0.0   \n",
       "Del Campo                              0.0       0.0     0.0      0.0   \n",
       "The Ashby Inn & Restaurant             0.0       0.0     0.0      0.0   \n",
       "Restaurant Eve                         0.0       0.0     0.0      0.0   \n",
       "The Restaurant at Patowmack Farm       0.0       0.0     0.0      0.0   \n",
       "Equinox                                0.0       0.0     0.0      0.0   \n",
       "\n",
       "                                  Brazilian  Burmese  ...   15873  15874  \\\n",
       "restaurant                                            ...                  \n",
       "Komi                                    0.0      0.0  ...     0.0    0.0   \n",
       "Little Serow                            0.0      0.0  ...     0.0    0.0   \n",
       "Plume                                   0.0      0.0  ...     0.0    0.0   \n",
       "The Inn at Little Washington            0.0      0.0  ...     0.0    0.0   \n",
       "The Source                              0.0      0.0  ...     0.0    0.0   \n",
       "Blue Duck Tavern                        0.0      0.0  ...     0.0    0.0   \n",
       "Minibar                                 0.0      0.0  ...     0.0    0.0   \n",
       "Del Campo                               0.0      0.0  ...     0.0    0.0   \n",
       "The Ashby Inn & Restaurant              0.0      0.0  ...     0.0    0.0   \n",
       "Restaurant Eve                          0.0      0.0  ...     0.0    0.0   \n",
       "The Restaurant at Patowmack Farm        0.0      0.0  ...     0.0    0.0   \n",
       "Equinox                                 0.0      0.0  ...     0.0    0.0   \n",
       "\n",
       "                                  15875  15876  15877  15878  15879  15880  \\\n",
       "restaurant                                                                   \n",
       "Komi                                0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "Little Serow                        0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "Plume                               0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "The Inn at Little Washington        0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "The Source                          0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "Blue Duck Tavern                    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "Minibar                             0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "Del Campo                           0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "The Ashby Inn & Restaurant          0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "Restaurant Eve                      0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "The Restaurant at Patowmack Farm    0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "Equinox                             0.0    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "                                  15881  pred  \n",
       "restaurant                                     \n",
       "Komi                                0.0   1.0  \n",
       "Little Serow                        0.0   1.0  \n",
       "Plume                               0.0   1.0  \n",
       "The Inn at Little Washington        0.0   1.0  \n",
       "The Source                          0.0   1.0  \n",
       "Blue Duck Tavern                    0.0   1.0  \n",
       "Minibar                             0.0   1.0  \n",
       "Del Campo                           0.0   1.0  \n",
       "The Ashby Inn & Restaurant          0.0   1.0  \n",
       "Restaurant Eve                      0.0   1.0  \n",
       "The Restaurant at Patowmack Farm    0.0   1.0  \n",
       "Equinox                             0.0   1.0  \n",
       "\n",
       "[12 rows x 15953 columns]"
      ]
     },
     "execution_count": 603,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wash_pred_2 = lr2.predict(WASH_X)\n",
    "WASH_X_LR2_PRED = WASH_X\n",
    "WASH_X_LR2_PRED['pred'] = wash_pred_2\n",
    "WASH_LR2_PRED = WASH_X_LR2_PRED.where(WASH_X_LR2_PRED['pred'] == 1)\n",
    "WASH_LR2_PRED.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Importance Using Random Forests \n",
    "- MICH_X\n",
    "- MICH_Y - Boolean Start\n",
    "- MICH_Y2 - Rating 0-3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=False, class_weight=None, criterion='gini',\n",
       "           max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=25, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 517,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting the random Forest Model\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "ETC = ExtraTreesClassifier(n_estimators=25)\n",
    "ETC.fit(MICH_X, MICH_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Getting the importance value of each of the feature.\n",
    "imp = ETC.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating a dataframe of the column(feature name) and its importance.\n",
    "imp_df = pd.DataFrame()\n",
    "imp_df['col'] = MICH_X.columns\n",
    "imp_df['importance'] = imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelstack/anaconda/lib/python2.7/site-packages/ipykernel/__main__.py:1: FutureWarning: sort(columns=....) is deprecated, use sort_values(by=.....)\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# Sorting the feature importance column and dropping the index\n",
    "imp_df.sort(['importance'], ascending = False, inplace = True)\n",
    "imp_df.reset_index(inplace=True, drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - \"imp_250_ls\" -  List of the column names of the 250 most important features according to Extra Trees Feature importance\n",
    "##### - \"imp_features_ls\" - List of the column names of features whose importance is greater than 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting the top 250 features and converting them to a list\n",
    "important_250 = imp_df[:250]\n",
    "imp_250_ls = important_250['col']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Getting all features whose importance is greater than 0\n",
    "imp_df =  imp_df[(imp_df['importance'] != 0)]\n",
    "imp_features_ls = imp_df['col']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### - IMP_MICH_X  - Dataframe with features that have an importance greater than 0\n",
    "##### - IMP_MICH_X_250 - Dataframe of only the top 250 features as far as predictive capability.\n",
    "##### - IMP_WASH_X - Test dataframe with only features whos imporatance is greater than 0\n",
    "#####  - IMP_WASH_X_250 - Test Dataframe with only the top 250 features as far as predictive capability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IMP_MICH_X = MICH_X[imp_features_ls]\n",
    "IMP_MICH_X_250 = MICH_X[imp_250_ls]\n",
    "\n",
    "IMP_WASH_X = WASH_X[imp_features_ls]\n",
    "IMP_WASH_X_250 = WASH_X[imp_250_ls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Washington Bib Gourmand Restaurants\n",
    "The List of Michelin Honerable Mention (Bib Gourmand) restaurants was released on Thursday October 6th.  There are only 19 restaurants which leads me to believe the that number of restaurants that will recieve a Michelin star will be less (around half).  Less than half of these appear in my test dataset which could be a good or a bad thing.  Either many of the restaurants in my test data are above a 'honerable mention' grad and are more likely to get stars, or I have created a great deal of bias by not having every posible restaurant in my test data.  \n",
    "\n",
    "I Can use this two ways for model tuning.  \n",
    "- Compare my results to these and make sure my model doesnt give these a star (or just take them off my final list). \n",
    "- Take their rows out of test dataset and put them into the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Michelin Bib Gourmand list \n",
    "'''https://www.washingtonpost.com/news/going-out-guide/wp/2016/10/06/\n",
    "michelin-announces-its-first-d-c-honors-the-bib-gourmand-list-of-affordable-restaurants/'''\n",
    "wash_bib = ['Bad Saint', 'Bidwell', 'Boqueria','Chercher', 'China Chilcano', \n",
    "            'Das', 'Doi Moi', 'Jaleo','Kyirisan', 'Lapis','Maketto',\n",
    "            'Ottoman Taverna', 'Oyamel','Pearl Dive Oyster Palace',\n",
    "            'The Red Hen','Royal','Thip Khao','2Amys','Zaytinya']\n",
    "\n",
    "# Bib Gourmand Restaurants in my test data.  \n",
    "wash_bib_ls = []\n",
    "for item in washington_100['restaurant']:\n",
    "    if item in wash_bib:\n",
    "        wash_bib_ls.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Extrating the Bib Gourmand restaurants from test data to incorporate encorporate into the Train dataset.  \n",
    "# surveys_df.loc[[0,10], :]\n",
    "WASH_BIB = WASH_X.loc[wash_bib_ls]\n",
    "WASH_BIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "WASH_X.where(WASH_X is in wash_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression 2: \n",
    "Logistic Regression is now worthless."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Standard data (Bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 718,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.925343811395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[443,  13],\n",
       "       [ 25,  28]])"
      ]
     },
     "execution_count": 718,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(MICH_X, MICH_Y, test_size = 0.3, random_state = 19)\n",
    "\n",
    "lr = LogisticRegression(penalty = 'l1')\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "pred_lr = lr.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, pred_lr)\n",
    "\n",
    "confusion_matrix(y_test, pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Standard data (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.907662082515\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[450,   6,   0,   0],\n",
       "       [ 26,  12,   0,   0],\n",
       "       [  2,   6,   0,   0],\n",
       "       [  1,   6,   0,   0]])"
      ]
     },
     "execution_count": 719,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(MICH_X, MICH_Y2, test_size = 0.3, random_state = 19)\n",
    "\n",
    "lr = LogisticRegression(penalty = 'l1')\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "pred_lr = lr.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, pred_lr)\n",
    "\n",
    "confusion_matrix(y_test, pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Selected Data (Bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 720,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.895874263261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[456,   0],\n",
       "       [ 53,   0]])"
      ]
     },
     "execution_count": 720,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(IMP_MICH_X, MICH_Y, test_size = 0.3, random_state = 19)\n",
    "\n",
    "lr = LogisticRegression(penalty = 'l1')\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "pred_lr = lr.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, pred_lr)\n",
    "\n",
    "confusion_matrix(y_test, pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Selected Data (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.895874263261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[456,   0,   0,   0],\n",
       "       [ 38,   0,   0,   0],\n",
       "       [  8,   0,   0,   0],\n",
       "       [  7,   0,   0,   0]])"
      ]
     },
     "execution_count": 721,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(IMP_MICH_X, MICH_Y2, test_size = 0.3, random_state = 19)\n",
    "\n",
    "lr = LogisticRegression(penalty = 'l1')\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "pred_lr = lr.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, pred_lr)\n",
    "\n",
    "confusion_matrix(y_test, pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Selected 250 Features (Bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.895874263261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[456,   0],\n",
       "       [ 53,   0]])"
      ]
     },
     "execution_count": 722,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(IMP_MICH_X_250, MICH_Y, test_size = 0.3, random_state = 19)\n",
    "\n",
    "lr = LogisticRegression(penalty = 'l1')\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "pred_lr = lr.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, pred_lr)\n",
    "\n",
    "confusion_matrix(y_test, pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Selected 250 Features  (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 723,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.895874263261\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[456,   0,   0,   0],\n",
       "       [ 38,   0,   0,   0],\n",
       "       [  8,   0,   0,   0],\n",
       "       [  7,   0,   0,   0]])"
      ]
     },
     "execution_count": 723,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(IMP_MICH_X_250, MICH_Y2, test_size = 0.3, random_state = 19)\n",
    "\n",
    "lr = LogisticRegression(penalty = 'l1')\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "pred_lr = lr.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, pred_lr)\n",
    "\n",
    "confusion_matrix(y_test, pred_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Kneighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Original Data (Bool)\n",
    "I will run a few with different k (5, 7 & 13) for EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5 Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 737,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.941060903733\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[455,  12],\n",
       "       [ 18,  24]])"
      ]
     },
     "execution_count": 737,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "X_train, X_test, y_train, y_test = train_test_split(MICH_X, MICH_Y, random_state = 18, test_size = 0.3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, y_pred)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7 Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 736,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.94695481336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[457,  10],\n",
       "       [ 17,  25]])"
      ]
     },
     "execution_count": 736,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 7)\n",
    "X_train, X_test, y_train, y_test = train_test_split(MICH_X, MICH_Y, random_state = 18, test_size = 0.3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, y_pred)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13 Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.939096267191\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[460,   7],\n",
       "       [ 24,  18]])"
      ]
     },
     "execution_count": 738,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 13)\n",
    "X_train, X_test, y_train, y_test = train_test_split(MICH_X, MICH_Y, random_state = 18, test_size = 0.3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, y_pred)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Original Data (Category)\n",
    "I will run a few with different k (5, 7 & 13) for EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5 Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.919449901768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[456,  10,   0,   1],\n",
       "       [ 18,  11,   0,   0],\n",
       "       [  3,   3,   1,   0],\n",
       "       [  1,   4,   1,   0]])"
      ]
     },
     "execution_count": 742,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5 Neighbors\n",
    "knn = KNeighborsClassifier()\n",
    "X_train, X_test, y_train, y_test = train_test_split(MICH_X, MICH_Y2, random_state = 18, test_size = 0.3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, y_pred)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 7 Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.919449901768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[457,  10,   0,   0],\n",
       "       [ 18,  11,   0,   0],\n",
       "       [  2,   5,   0,   0],\n",
       "       [  1,   3,   2,   0]])"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 7)\n",
    "X_train, X_test, y_train, y_test = train_test_split(MICH_X, MICH_Y2, random_state = 18, test_size = 0.3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, y_pred)\n",
    "confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 13 Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 739,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.92141453831\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[460,   7,   0,   0],\n",
       "       [ 20,   9,   0,   0],\n",
       "       [  3,   4,   0,   0],\n",
       "       [  1,   4,   1,   0]])"
      ]
     },
     "execution_count": 739,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 13)\n",
    "X_train, X_test, y_train, y_test = train_test_split(MICH_X, MICH_Y2, random_state = 18, test_size = 0.3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, y_pred)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Selected Data (Bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 732,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.917485265226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[467,   0],\n",
       "       [ 42,   0]])"
      ]
     },
     "execution_count": 732,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 7)\n",
    "X_train, X_test, y_train, y_test = train_test_split(IMP_MICH_X, MICH_Y, random_state = 18, test_size = 0.3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, y_pred)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Selected Data (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 733,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.917485265226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[467,   0,   0,   0],\n",
       "       [ 29,   0,   0,   0],\n",
       "       [  7,   0,   0,   0],\n",
       "       [  6,   0,   0,   0]])"
      ]
     },
     "execution_count": 733,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 7)\n",
    "X_train, X_test, y_train, y_test = train_test_split(IMP_MICH_X, MICH_Y2, random_state = 18, test_size = 0.3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, y_pred)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Selected Data 250 (Bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 734,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.923379174853\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[467,   0],\n",
       "       [ 39,   3]])"
      ]
     },
     "execution_count": 734,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 7)\n",
    "X_train, X_test, y_train, y_test = train_test_split(IMP_MICH_X_250, MICH_Y, random_state = 18, test_size = 0.3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, y_pred)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN Selected Data 250 (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 735,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.917485265226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[467,   0,   0,   0],\n",
       "       [ 29,   0,   0,   0],\n",
       "       [  6,   1,   0,   0],\n",
       "       [  6,   0,   0,   0]])"
      ]
     },
     "execution_count": 735,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 7)\n",
    "X_train, X_test, y_train, y_test = train_test_split(IMP_MICH_X_250, MICH_Y2, random_state = 18, test_size = 0.3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, y_pred)\n",
    "confusion_matrix(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM.SVC 2: Using Selected Features.\n",
    "#### Support Vector Machine Support Vector Classification\n",
    "- MICH_X\n",
    "- IMP_MICH_X, \n",
    "- IMP_MICH_X_250"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine with Standard data (Bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.913555992141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[465,   0],\n",
       "       [ 44,   0]])"
      ]
     },
     "execution_count": 726,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(MICH_X, MICH_Y, test_size = 0.3, random_state = 22)\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "svm_y_pred = clf.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, svm_y_pred)\n",
    "confusion_matrix(y_test, svm_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine with Standard data (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 727,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.913555992141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[465,   0,   0,   0],\n",
       "       [ 37,   0,   0,   0],\n",
       "       [  3,   0,   0,   0],\n",
       "       [  4,   0,   0,   0]])"
      ]
     },
     "execution_count": 727,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(MICH_X, MICH_Y2, test_size = 0.3, random_state = 22)\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "svm_y_pred = clf.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, svm_y_pred)\n",
    "confusion_matrix(y_test, svm_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine with Selected Features (Bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 731,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.913555992141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[465,   0],\n",
       "       [ 44,   0]])"
      ]
     },
     "execution_count": 731,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(IMP_MICH_X, MICH_Y, test_size = 0.3, random_state = 22)\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "svm_y_pred = clf.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, svm_y_pred)\n",
    "confusion_matrix(y_test, svm_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine with Selected Features (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 728,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.913555992141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[465,   0,   0,   0],\n",
       "       [ 37,   0,   0,   0],\n",
       "       [  3,   0,   0,   0],\n",
       "       [  4,   0,   0,   0]])"
      ]
     },
     "execution_count": 728,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(IMP_MICH_X, MICH_Y2, test_size = 0.3, random_state = 22)\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "svm_y_pred = clf.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, svm_y_pred)\n",
    "confusion_matrix(y_test, svm_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine with Selected Features 250 (Bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 730,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.913555992141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[465,   0],\n",
       "       [ 44,   0]])"
      ]
     },
     "execution_count": 730,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(IMP_MICH_X_250, MICH_Y, test_size = 0.3, random_state = 22)\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "svm_y_pred = clf.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, svm_y_pred)\n",
    "confusion_matrix(y_test, svm_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine with Selected Features 250 (Categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 729,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.913555992141\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[465,   0,   0,   0],\n",
       "       [ 37,   0,   0,   0],\n",
       "       [  3,   0,   0,   0],\n",
       "       [  4,   0,   0,   0]])"
      ]
     },
     "execution_count": 729,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(IMP_MICH_X_250, MICH_Y2, test_size = 0.3, random_state = 22)\n",
    "clf = svm.SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "svm_y_pred = clf.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, svm_y_pred)\n",
    "confusion_matrix(y_test, svm_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### Current findings\n",
    "\n",
    "I have built several models so far and one thing is definitely conclussive.  My selected features are terrible!  The predictive power of my models drop off significantly after I incorporate selected features.  It looks like my KNN model using the Original data and predicting a categorical outcome is performing the best.  Specifically the one using 7 neighbors.  I will need to run an optimization function (Gridsearch) to see what the optimal parameters are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 751,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.919449901768\n",
      "0.908998866956\n",
      "0.898788337128\n",
      "0.919449901768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samuelstack/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/samuelstack/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/Users/samuelstack/anaconda/lib/python2.7/site-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[457,  10,   0,   0],\n",
       "       [ 18,  11,   0,   0],\n",
       "       [  2,   5,   0,   0],\n",
       "       [  1,   3,   2,   0]])"
      ]
     },
     "execution_count": 751,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors = 7)\n",
    "X_train, X_test, y_train, y_test = train_test_split(MICH_X, MICH_Y2, random_state = 18, test_size = 0.3)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "print accuracy_score(y_test, y_pred)\n",
    "print f1_score(y_test, y_pred)\n",
    "#print log_loss(y_test, y_pred)\n",
    "print precision_score(y_test, y_pred)\n",
    "print recall_score(y_test, y_pred)\n",
    "#print roc_auc_score(y_test, y_pred) 'Multiclass format not supported'\n",
    "confusion_matrix(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimized Parameters with GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a dataset of the size that I have, the model I am using and the parameters I am searching, it is unreasonable to ask my local device to try to perform this task.  I tried and it ran for 2 hours without before I realized I was using the wrong Y values.  Alternatively, I am going to set up an AWS instance and use that to run my GridSearchCV.  I will outline what I will be doing on the AWS instance below so process and methods do not get lost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 755,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_jobs= -1)\n",
    "\n",
    "params = {'n_neighbors':[3,5,7,9,11,13], 'weights': ['uniform','distance'], 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'], \n",
    "          'leaf_size' : [10,20,30,40,50]}\n",
    "\n",
    "gs_knn = GridSearchCV(knn, params, scoring = )\n",
    "\n",
    "gs_knn.fit(MICH_X, MICH_Y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not alot of code for something that is extremely computationaly expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors = 13, leaf_size = 10, weights = 'distance', n_jobs = -1)\n",
    "KNN.fit(MICH_X, MICH_Y2)\n",
    "KNN.predict(WASH_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNN = KNeighborsClassifier(n_neighbors = 13, leaf_size = 10, weights = 'distance', n_jobs = -1)\n",
    "KNN.fit(MICH_X, MICH_Y)\n",
    "wash_pred = KNN.predict(WASH_X)\n",
    "wash_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "WASH_X['pred'] = wash_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final = WASH_X['pred'].where(WASH_X['pred'] == 1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df = pd.DataFrame(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restaurant</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Fiola Mare</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Masseria</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fiola</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Preserve</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Obelisk</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Del Campo</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Woodberry Kitchen</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Centrolina</th>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   pred\n",
       "restaurant             \n",
       "Fiola Mare          1.0\n",
       "Masseria            1.0\n",
       "Fiola               1.0\n",
       "Preserve            1.0\n",
       "Obelisk             1.0\n",
       "Del Campo           1.0\n",
       "Woodberry Kitchen   1.0\n",
       "Centrolina          1.0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My models could not predict number of starts, even the KNN classifier with train data with 4 catagories, so this is my list of restaurants that will get starts for dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "final_df.to_csv('MichelinPredictions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
