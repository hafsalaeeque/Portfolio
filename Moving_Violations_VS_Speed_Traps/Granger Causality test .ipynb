{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Yearly Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Aggregated moving violations data concatted in previous notebook\n",
    "mv2011 = pd.read_csv('Data/Moving Violation data year/moving_violations2011.csv')\n",
    "mv2012 = pd.read_csv('Data/Moving Violation data year/moving_violations2012.csv')\n",
    "mv2013 = pd.read_csv('Data/Moving Violation data year/moving_violations2013.csv')\n",
    "mv2014 = pd.read_csv('Data/Moving Violation data year/moving_violations2014.csv')\n",
    "mv2015 = pd.read_csv('Data/Moving Violation data year/moving_violations2015.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>TICKETTYPE</th>\n",
       "      <th>TICKETISSUEDATE</th>\n",
       "      <th>TRAP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>-77.088404</td>\n",
       "      <td>38.915716</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2012-01-02T00:00:00.000Z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-76.952187</td>\n",
       "      <td>38.895681</td>\n",
       "      <td>Photo</td>\n",
       "      <td>2012-01-02T00:00:00.000Z</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  LONGITUDE   LATITUDE TICKETTYPE           TICKETISSUEDATE  TRAP\n",
       "0           0 -77.088404  38.915716      Photo  2012-01-02T00:00:00.000Z     1\n",
       "1           1 -76.952187  38.895681      Photo  2012-01-02T00:00:00.000Z     1"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data example head\n",
    "mv2012.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Crash Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "crashes = pd.read_csv('Data/Crashes_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge all Yearly ticket data into one data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tickets = mv2011.append([mv2012, mv2013, mv2014, mv2015])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop extra row\n",
    "tickets.drop('Unnamed: 0', axis =1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only take rows where the ticket type is 'Photo' (engineer column 'TRAP' also does the same)\n",
    "traps = tickets[tickets['TRAP'] == 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " What is the proportion of Photo Tickets to Hand Written Tickets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9356710367210453"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(len(traps))/float(len(tickets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "93.5% of all tickets given from 2011 to 2015 are from photo traps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing the  Time feature from tickets so I am left with only a date\n",
    "Counts of daily occurances are the lowest level by which this model is going to assess the date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ticketdate_list = traps['TICKETISSUEDATE'].tolist()\n",
    "\n",
    "dtl = []\n",
    "for date in ticketdate_list:\n",
    "    new = date.replace('T00:00:00.000Z','')\n",
    "\n",
    "    dtl.append(new)\n",
    "    \n",
    "traps['DATE'] = dtl\n",
    "\n",
    "traps.drop('TICKETISSUEDATE', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crash = crashes[['LONGITUDE','LATITUDE','DATE', 'EVENT']]\n",
    "\n",
    "# Converting the date time object to just a date column. \n",
    "crash_t = pd.DataFrame(crash['DATE'].str.split(' ',1).tolist(), columns = ['date','time'])\n",
    "\n",
    "day = crash_t['date']\n",
    "\n",
    "crash.drop('DATE', axis = 1, inplace = True)\n",
    "\n",
    "crash['DATE']= day\n",
    "\n",
    "# Dropping null values\n",
    "crash.dropna(inplace = True)\n",
    "\n",
    "# removing dates outside the range of 2011 - 2015\n",
    "crash['DATE'] = pd.to_datetime(crash['DATE'])\n",
    "crash = crash[(crashes['DATE'].dt.year >= 2011)]\n",
    "crash = crash[(crashes['DATE'].dt.year <= 2015)]\n",
    "\n",
    "# dont care about the error and depreciation message.  It did what i needed it to."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding a 'Crash' feature so i can concat easily\n",
    "traps.drop('TICKETTYPE',axis = 1, inplace = True)\n",
    "traps['CRASH'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adding a 'Trap' feature so i can concat easily\n",
    "crash['EVENT'] = 1\n",
    "crash.rename(columns = {'EVENT':'CRASH'}, inplace = True)\n",
    "crash['TRAP'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both of my dataframes have the same column structure.  Now I can add them together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theory = pd.concat([crash, traps])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "truncate lat and longitude for the purpose of this experiment and then do a grouped by count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Date Column needed an adjustment\n",
    "theory['DATE'] = pd.to_datetime(theory['DATE'], format = '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# exporting data to attempt a DB scan using AWS considering how long it would take.\n",
    "theory.to_csv('dbscan_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# round datetime if needed.  \n",
    "#theory[['LONGITUDE','LATITUDE']] = theory[['LONGITUDE','LATITUDE']].round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Creating individual sub dataframes by year.  \n",
    "theory2011 = theory[(theory['DATE'].dt.year == 2011)]\n",
    "theory2012 = theory[(theory['DATE'].dt.year == 2012)]\n",
    "theory2013 = theory[(theory['DATE'].dt.year == 2013)]\n",
    "theory2014 = theory[(theory['DATE'].dt.year == 2014)]\n",
    "theory2015 = theory[(theory['DATE'].dt.year == 2015)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2958211\n",
       "1     123070\n",
       "Name: CRASH, dtype: int64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking prior\n",
    "theory['CRASH'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_count = theory.groupby(['LONGITUDE', 'LATITUDE','DATE'],as_index = False).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Looks like this was a failure.  No way 1740 crashes occured on one day.  Its just counting all the observations everyday.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Granger Causaility Test\n",
    "My Kernal Died a few times while troubleshooting this function so i included all the importand and necessary cleaning needed in the two cells below.\n",
    "I also ran though a trial experiment with one year of data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.stattools import grangercausalitytests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theory = pd.read_csv('dbscan_data.csv')\n",
    "theory.drop('Unnamed: 0', axis =1, inplace = True)\n",
    "theory['DATE'] = pd.to_datetime(theory['DATE'], format = '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2011 Trial.  This runs pretty fast im going to up it to 5 years of data.  \n",
    "# theory2011 = theory[(theory['DATE'].dt.year == 2011)]\n",
    "\n",
    "# theory2011['LATITUDE'] = theory2011['LATITUDE'].round(3)\n",
    "# theory2011['LATITUDE'] = theory2011['LATITUDE'].astype(str)\n",
    "\n",
    "# theory2011['LONGITUDE'] = theory2011['LONGITUDE'].round(3)\n",
    "# theory2011['LONGITUDE'] = theory2011['LONGITUDE'].astype(str)\n",
    "\n",
    "# theory2011['location'] = theory2011['LATITUDE']+', '+theory2011['LONGITUDE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRASH</th>\n",
       "      <th>DATE</th>\n",
       "      <th>LATITUDE</th>\n",
       "      <th>LONGITUDE</th>\n",
       "      <th>TRAP</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>38.888</td>\n",
       "      <td>-76.932</td>\n",
       "      <td>0</td>\n",
       "      <td>38.888, -76.932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-01</td>\n",
       "      <td>38.933</td>\n",
       "      <td>-76.973</td>\n",
       "      <td>0</td>\n",
       "      <td>38.933, -76.973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-08</td>\n",
       "      <td>38.906</td>\n",
       "      <td>-77.045</td>\n",
       "      <td>0</td>\n",
       "      <td>38.906, -77.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>2008-08</td>\n",
       "      <td>38.887</td>\n",
       "      <td>-76.937</td>\n",
       "      <td>0</td>\n",
       "      <td>38.887, -76.937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>2009-06</td>\n",
       "      <td>38.927</td>\n",
       "      <td>-77.034</td>\n",
       "      <td>0</td>\n",
       "      <td>38.927, -77.034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   CRASH     DATE LATITUDE LONGITUDE  TRAP         location\n",
       "0      1  2009-01   38.888   -76.932     0  38.888, -76.932\n",
       "1      1  2009-01   38.933   -76.973     0  38.933, -76.973\n",
       "2      1  2008-08   38.906   -77.045     0  38.906, -77.045\n",
       "3      1  2008-08   38.887   -76.937     0  38.887, -76.937\n",
       "4      1  2009-06   38.927   -77.034     0  38.927, -77.034"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Per Joe and Haley I need to aggregate by the month\n",
    "theory['DATE'] = theory.DATE.map(lambda x: x.strftime('%Y-%m'))\n",
    "theory.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theory['LATITUDE'] = theory['LATITUDE'].round(3)\n",
    "theory['LATITUDE'] = theory['LATITUDE'].astype(str)\n",
    "\n",
    "theory['LONGITUDE'] = theory['LONGITUDE'].round(3)\n",
    "theory['LONGITUDE'] = theory['LONGITUDE'].astype(str)\n",
    "\n",
    "theory['location'] = theory['LATITUDE']+', '+theory['LONGITUDE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "locations_ua = theory['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# locations_u = theory2011['location'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COLS = ['Location',\n",
    "        'lag1_Ftest','lag1_CHI2','lag1_LR','lag1_params_Ftest',\n",
    "        'lag2_Ftest','lag2_CHI2','lag2_LR','lag2_params_Ftest',\n",
    "        'lag3_Ftest','lag3_CHI2','lag3_LR','lag3_params_Ftest',\n",
    "        'lag4_Ftest','lag4_CHI2','lag4_LR','lag4_params_Ftest',\n",
    "        'lag5_Ftest','lag5_CHI2','lag5_LR','lag5_params_Ftest',\n",
    "        'lag6_Ftest','lag6_CHI2','lag6_LR','lag6_params_Ftest']\n",
    "granger_results_df = pd.DataFrame(columns = COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# already is a list of locations that have ALREADY been passed through the function and if they\n",
    "# were good values it will have already populated the dataframe and bad ones will be there to\n",
    "#skip over\n",
    "already=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GCT Function\n",
    "- This function pulls from my dataframe of all ticket and crash records based on a list of unique locations (String of Lat and Lon).\n",
    "- Checks to make sure there is at least one crash record and at least 2 ticket records.\n",
    "- Splits data into two dataframes. One for Crashes and one for Traps\n",
    "- Drops any remaining zero value records.\n",
    "- Counts up the observations to aggregate at the monthly level.\n",
    "- Sorts DFs by data.\n",
    "- Rejoins or date and sorts again on date\n",
    "- Passes resulting dataframe through the StatsModel Granger Causality Test.\n",
    "- Appends results to a Dataframe\n",
    "\n",
    "Sometimes values make it through that can not be passed through the GCT and cause the loop to distrupt.  \n",
    "This occurs if there are not enough observations to calculate the max lag (only 5 months of observations trying to calculate a cause on 6 months of lag.  I created a work around that appends every location that has been passed to a list and then run a 'if x is not in y_list:' proceed with function type deal so i could just keep running the GCT over and over and over again and appending new results to my dataframe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gct_cal(trial):\n",
    "    # use location to pull all relevant observations from teh Theory2011 DF\n",
    "    trial_loc_df = theory[['DATE','CRASH','TRAP']][(theory['location'] == trial)]\n",
    "    already.append(trial)\n",
    "    # check if the location has values for both 'CRASH' and 'TRAP'\n",
    "    if trial_loc_df['CRASH'].sum() > 0:\n",
    "\n",
    "        if trial_loc_df['TRAP'].sum() > 1:\n",
    "            print(trial)\n",
    "            # Split dataframe\n",
    "            trial_df1 = pd.DataFrame(trial_loc_df[['DATE','CRASH']])\n",
    "            trial_df2 = pd.DataFrame(trial_loc_df[['DATE','TRAP']])\n",
    "            \n",
    "            # dropping all values with no record\n",
    "            trial_df1a =trial_df1[trial_df1['CRASH'] != 0]\n",
    "            trial_df2a =trial_df2[trial_df2['TRAP'] != 0]\n",
    "            \n",
    "            # going to have to get daily counts to make indexes unique values by taking the counts for each day\n",
    "            crash_loc = trial_df1a.groupby('DATE').count()\n",
    "            trap_loc = trial_df2a.groupby('DATE').count()\n",
    "            \n",
    "            # remerging dataframes together and filling nulls with 0's\n",
    "            # Switch around the position of 'crash_loc' and 'trap_loc' to see the result of one on the other.\n",
    "            trial_result = pd.concat([crash_loc,trap_loc], axis =1)\n",
    "            trial_result.fillna(value = 0, inplace = True)\n",
    "            \n",
    "            trial_result.sort_index(inplace = True)\n",
    "            if len(trial_result) >= 6:\n",
    "    \n",
    "            #running the GCT\n",
    "                gct = grangercausalitytests(trial_result, maxlag = 6)\n",
    "             \n",
    "            \n",
    "            # getting metric values to append to a dataframe\n",
    "                locale = trial\n",
    "                lag1_ssr_Ftest = gct[1][0]['ssr_ftest']\n",
    "                lag1_ssr_CHI2 = gct[1][0]['ssr_chi2test']\n",
    "                lag1_LR = gct[1][0]['lrtest']\n",
    "                lag1_params_Ftest = gct[1][0]['params_ftest']\n",
    "\n",
    "                lag2_ssr_Ftest = gct[2][0]['ssr_ftest']\n",
    "                lag2_ssr_CHI2 = gct[2][0]['ssr_chi2test']\n",
    "                lag2_LR = gct[2][0]['lrtest']\n",
    "                lag2_params_Ftest = gct[2][0]['params_ftest']\n",
    "\n",
    "                lag3_ssr_Ftest = gct[3][0]['ssr_ftest']\n",
    "                lag3_ssr_CHI2 = gct[3][0]['ssr_chi2test']\n",
    "                lag3_LR = gct[3][0]['lrtest']\n",
    "                lag3_params_Ftest = gct[3][0]['params_ftest']\n",
    "            \n",
    "                lag4_ssr_Ftest = gct[4][0]['ssr_ftest']\n",
    "                lag4_ssr_CHI2 = gct[4][0]['ssr_chi2test']\n",
    "                lag4_LR = gct[4][0]['lrtest']\n",
    "                lag4_params_Ftest = gct[4][0]['params_ftest']\n",
    "            \n",
    "                lag5_ssr_Ftest = gct[5][0]['ssr_ftest']\n",
    "                lag5_ssr_CHI2 = gct[5][0]['ssr_chi2test']\n",
    "                lag5_LR = gct[5][0]['lrtest']\n",
    "                lag5_params_Ftest = gct[3][0]['params_ftest']\n",
    "            \n",
    "                lag6_ssr_Ftest = gct[6][0]['ssr_ftest']\n",
    "                lag6_ssr_CHI2 = gct[6][0]['ssr_chi2test']\n",
    "                lag6_LR = gct[6][0]['lrtest']\n",
    "                lag6_params_Ftest = gct[6][0]['params_ftest']\n",
    "            \n",
    "                # appending information to dataframe\n",
    "                granger_results_df.loc[len(granger_results_df)]=[locale, \n",
    "                                                            lag1_ssr_Ftest, lag1_ssr_CHI2, lag1_LR, lag1_params_Ftest,\n",
    "                                                            lag2_ssr_Ftest, lag2_ssr_CHI2, lag2_LR, lag2_params_Ftest,\n",
    "                                                            lag3_ssr_Ftest, lag3_ssr_CHI2, lag3_LR, lag3_params_Ftest,\n",
    "                                                            lag4_ssr_Ftest, lag4_ssr_CHI2, lag4_LR, lag4_params_Ftest,\n",
    "                                                            lag5_ssr_Ftest, lag5_ssr_CHI2, lag5_LR, lag5_params_Ftest,\n",
    "                                                            lag6_ssr_Ftest, lag6_ssr_CHI2, lag6_LR, lag6_params_Ftest] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.892, -77.028\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 1)\n",
      "ssr based F test:         F=0.2299  , p=0.6333  , df_denom=63, df_num=1\n",
      "ssr based chi2 test:   chi2=0.2408  , p=0.6236  , df=1\n",
      "likelihood ratio test: chi2=0.2404  , p=0.6239  , df=1\n",
      "parameter F test:         F=0.2299  , p=0.6333  , df_denom=63, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 2)\n",
      "ssr based F test:         F=0.1044  , p=0.9010  , df_denom=60, df_num=2\n",
      "ssr based chi2 test:   chi2=0.2262  , p=0.8931  , df=2\n",
      "likelihood ratio test: chi2=0.2258  , p=0.8932  , df=2\n",
      "parameter F test:         F=0.1044  , p=0.9010  , df_denom=60, df_num=2\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 3)\n",
      "ssr based F test:         F=1.3013  , p=0.2829  , df_denom=57, df_num=3\n",
      "ssr based chi2 test:   chi2=4.3834  , p=0.2229  , df=3\n",
      "likelihood ratio test: chi2=4.2399  , p=0.2367  , df=3\n",
      "parameter F test:         F=1.3013  , p=0.2829  , df_denom=57, df_num=3\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 4)\n",
      "ssr based F test:         F=1.6506  , p=0.1750  , df_denom=54, df_num=4\n",
      "ssr based chi2 test:   chi2=7.7027  , p=0.1031  , df=4\n",
      "likelihood ratio test: chi2=7.2670  , p=0.1224  , df=4\n",
      "parameter F test:         F=1.6506  , p=0.1750  , df_denom=54, df_num=4\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 5)\n",
      "ssr based F test:         F=1.2857  , p=0.2846  , df_denom=51, df_num=5\n",
      "ssr based chi2 test:   chi2=7.8148  , p=0.1667  , df=5\n",
      "likelihood ratio test: chi2=7.3601  , p=0.1952  , df=5\n",
      "parameter F test:         F=1.2857  , p=0.2846  , df_denom=51, df_num=5\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 6)\n",
      "ssr based F test:         F=1.1317  , p=0.3585  , df_denom=48, df_num=6\n",
      "ssr based chi2 test:   chi2=8.6290  , p=0.1955  , df=6\n",
      "likelihood ratio test: chi2=8.0707  , p=0.2330  , df=6\n",
      "parameter F test:         F=1.1317  , p=0.3585  , df_denom=48, df_num=6\n",
      "38.866, -76.95\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 1)\n",
      "ssr based F test:         F=0.0422  , p=0.8381  , df_denom=51, df_num=1\n",
      "ssr based chi2 test:   chi2=0.0447  , p=0.8326  , df=1\n",
      "likelihood ratio test: chi2=0.0447  , p=0.8326  , df=1\n",
      "parameter F test:         F=0.0422  , p=0.8381  , df_denom=51, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 2)\n",
      "ssr based F test:         F=0.3871  , p=0.6811  , df_denom=48, df_num=2\n",
      "ssr based chi2 test:   chi2=0.8549  , p=0.6522  , df=2\n",
      "likelihood ratio test: chi2=0.8481  , p=0.6544  , df=2\n",
      "parameter F test:         F=0.3871  , p=0.6811  , df_denom=48, df_num=2\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 3)\n",
      "ssr based F test:         F=1.3505  , p=0.2700  , df_denom=45, df_num=3\n",
      "ssr based chi2 test:   chi2=4.6817  , p=0.1966  , df=3\n",
      "likelihood ratio test: chi2=4.4828  , p=0.2138  , df=3\n",
      "parameter F test:         F=1.3505  , p=0.2700  , df_denom=45, df_num=3\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 4)\n",
      "ssr based F test:         F=1.1259  , p=0.3573  , df_denom=42, df_num=4\n",
      "ssr based chi2 test:   chi2=5.4688  , p=0.2425  , df=4\n",
      "likelihood ratio test: chi2=5.1950  , p=0.2679  , df=4\n",
      "parameter F test:         F=1.1259  , p=0.3573  , df_denom=42, df_num=4\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 5)\n",
      "ssr based F test:         F=1.1499  , p=0.3510  , df_denom=39, df_num=5\n",
      "ssr based chi2 test:   chi2=7.3710  , p=0.1945  , df=5\n",
      "likelihood ratio test: chi2=6.8758  , p=0.2300  , df=5\n",
      "parameter F test:         F=1.1499  , p=0.3510  , df_denom=39, df_num=5\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 6)\n",
      "ssr based F test:         F=1.2677  , p=0.2965  , df_denom=36, df_num=6\n",
      "ssr based chi2 test:   chi2=10.3530 , p=0.1106  , df=6\n",
      "likelihood ratio test: chi2=9.3924  , p=0.1527  , df=6\n",
      "parameter F test:         F=1.2677  , p=0.2965  , df_denom=36, df_num=6\n",
      "38.869, -76.969\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 1)\n",
      "ssr based F test:         F=4.4784  , p=0.0494  , df_denom=17, df_num=1\n",
      "ssr based chi2 test:   chi2=5.2687  , p=0.0217  , df=1\n",
      "likelihood ratio test: chi2=4.6767  , p=0.0306  , df=1\n",
      "parameter F test:         F=4.4784  , p=0.0494  , df_denom=17, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 2)\n",
      "ssr based F test:         F=3.4625  , p=0.0600  , df_denom=14, df_num=2\n",
      "ssr based chi2 test:   chi2=9.3982  , p=0.0091  , df=2\n",
      "likelihood ratio test: chi2=7.6359  , p=0.0220  , df=2\n",
      "parameter F test:         F=3.4625  , p=0.0600  , df_denom=14, df_num=2\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 3)\n",
      "ssr based F test:         F=2.7095  , p=0.0962  , df_denom=11, df_num=3\n",
      "ssr based chi2 test:   chi2=13.3011 , p=0.0040  , df=3\n",
      "likelihood ratio test: chi2=9.9590  , p=0.0189  , df=3\n",
      "parameter F test:         F=2.7095  , p=0.0962  , df_denom=11, df_num=3\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 4)\n",
      "ssr based F test:         F=1.7804  , p=0.2259  , df_denom=8, df_num=4\n",
      "ssr based chi2 test:   chi2=15.1334 , p=0.0044  , df=4\n",
      "likelihood ratio test: chi2=10.8236 , p=0.0286  , df=4\n",
      "parameter F test:         F=1.7804  , p=0.2259  , df_denom=8, df_num=4\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 5)\n",
      "ssr based F test:         F=1.2662  , p=0.4010  , df_denom=5, df_num=5\n",
      "ssr based chi2 test:   chi2=20.2600 , p=0.0011  , df=5\n",
      "likelihood ratio test: chi2=13.0900 , p=0.0225  , df=5\n",
      "parameter F test:         F=1.2662  , p=0.4010  , df_denom=5, df_num=5\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 6)\n",
      "ssr based F test:         F=1.8793  , p=0.3873  , df_denom=2, df_num=6\n",
      "ssr based chi2 test:   chi2=84.5687 , p=0.0000  , df=6\n",
      "likelihood ratio test: chi2=28.3920 , p=0.0001  , df=6\n",
      "parameter F test:         F=1.8793  , p=0.3873  , df_denom=2, df_num=6\n",
      "38.903, -77.056\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 1)\n",
      "ssr based F test:         F=3.5738  , p=0.0630  , df_denom=67, df_num=1\n",
      "ssr based chi2 test:   chi2=3.7338  , p=0.0533  , df=1\n",
      "likelihood ratio test: chi2=3.6376  , p=0.0565  , df=1\n",
      "parameter F test:         F=3.5738  , p=0.0630  , df_denom=67, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 2)\n",
      "ssr based F test:         F=1.6221  , p=0.2055  , df_denom=64, df_num=2\n",
      "ssr based chi2 test:   chi2=3.4976  , p=0.1740  , df=2\n",
      "likelihood ratio test: chi2=3.4118  , p=0.1816  , df=2\n",
      "parameter F test:         F=1.6221  , p=0.2055  , df_denom=64, df_num=2\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 3)\n",
      "ssr based F test:         F=1.9907  , p=0.1248  , df_denom=61, df_num=3\n",
      "ssr based chi2 test:   chi2=6.6573  , p=0.0837  , df=3\n",
      "likelihood ratio test: chi2=6.3512  , p=0.0957  , df=3\n",
      "parameter F test:         F=1.9907  , p=0.1248  , df_denom=61, df_num=3\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 4)\n",
      "ssr based F test:         F=1.9280  , p=0.1179  , df_denom=58, df_num=4\n",
      "ssr based chi2 test:   chi2=8.9085  , p=0.0634  , df=4\n",
      "likelihood ratio test: chi2=8.3640  , p=0.0791  , df=4\n",
      "parameter F test:         F=1.9280  , p=0.1179  , df_denom=58, df_num=4\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 5)\n",
      "ssr based F test:         F=1.5432  , p=0.1917  , df_denom=55, df_num=5\n",
      "ssr based chi2 test:   chi2=9.2592  , p=0.0992  , df=5\n",
      "likelihood ratio test: chi2=8.6647  , p=0.1232  , df=5\n",
      "parameter F test:         F=1.5432  , p=0.1917  , df_denom=55, df_num=5\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 6)\n",
      "ssr based F test:         F=1.2393  , p=0.3018  , df_denom=52, df_num=6\n",
      "ssr based chi2 test:   chi2=9.2946  , p=0.1577  , df=6\n",
      "likelihood ratio test: chi2=8.6873  , p=0.1919  , df=6\n",
      "parameter F test:         F=1.2393  , p=0.3018  , df_denom=52, df_num=6\n",
      "38.877, -76.965\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 1)\n",
      "ssr based F test:         F=5.5972  , p=0.0277  , df_denom=21, df_num=1\n",
      "ssr based chi2 test:   chi2=6.3968  , p=0.0114  , df=1\n",
      "likelihood ratio test: chi2=5.6708  , p=0.0172  , df=1\n",
      "parameter F test:         F=5.5972  , p=0.0277  , df_denom=21, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 2)\n",
      "ssr based F test:         F=4.0077  , p=0.0363  , df_denom=18, df_num=2\n",
      "ssr based chi2 test:   chi2=10.2420 , p=0.0060  , df=2\n",
      "likelihood ratio test: chi2=8.4714  , p=0.0145  , df=2\n",
      "parameter F test:         F=4.0077  , p=0.0363  , df_denom=18, df_num=2\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 3)\n",
      "ssr based F test:         F=3.6512  , p=0.0371  , df_denom=15, df_num=3\n",
      "ssr based chi2 test:   chi2=16.0655 , p=0.0011  , df=3\n",
      "likelihood ratio test: chi2=12.0618 , p=0.0072  , df=3\n",
      "parameter F test:         F=3.6512  , p=0.0371  , df_denom=15, df_num=3\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 4)\n",
      "ssr based F test:         F=3.3741  , p=0.0454  , df_denom=12, df_num=4\n",
      "ssr based chi2 test:   chi2=23.6188 , p=0.0001  , df=4\n",
      "likelihood ratio test: chi2=15.8263 , p=0.0033  , df=4\n",
      "parameter F test:         F=3.3741  , p=0.0454  , df_denom=12, df_num=4\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 5)\n",
      "ssr based F test:         F=1.9728  , p=0.1772  , df_denom=9, df_num=5\n",
      "ssr based chi2 test:   chi2=21.9200 , p=0.0005  , df=5\n",
      "likelihood ratio test: chi2=14.8006 , p=0.0112  , df=5\n",
      "parameter F test:         F=1.9728  , p=0.1772  , df_denom=9, df_num=5\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 6)\n",
      "ssr based F test:         F=3.1503  , p=0.0942  , df_denom=6, df_num=6\n",
      "ssr based chi2 test:   chi2=59.8549 , p=0.0000  , df=6\n",
      "likelihood ratio test: chi2=27.0402 , p=0.0001  , df=6\n",
      "parameter F test:         F=3.1503  , p=0.0942  , df_denom=6, df_num=6\n",
      "38.906, -76.998\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 1)\n",
      "ssr based F test:         F=2.0069  , p=0.1632  , df_denom=47, df_num=1\n",
      "ssr based chi2 test:   chi2=2.1350  , p=0.1440  , df=1\n",
      "likelihood ratio test: chi2=2.0907  , p=0.1482  , df=1\n",
      "parameter F test:         F=2.0069  , p=0.1632  , df_denom=47, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 2)\n",
      "ssr based F test:         F=1.5403  , p=0.2257  , df_denom=44, df_num=2\n",
      "ssr based chi2 test:   chi2=3.4307  , p=0.1799  , df=2\n",
      "likelihood ratio test: chi2=3.3159  , p=0.1905  , df=2\n",
      "parameter F test:         F=1.5403  , p=0.2257  , df_denom=44, df_num=2\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 3)\n",
      "ssr based F test:         F=0.9005  , p=0.4492  , df_denom=41, df_num=3\n",
      "ssr based chi2 test:   chi2=3.1629  , p=0.3672  , df=3\n",
      "likelihood ratio test: chi2=3.0631  , p=0.3820  , df=3\n",
      "parameter F test:         F=0.9005  , p=0.4492  , df_denom=41, df_num=3\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 4)\n",
      "ssr based F test:         F=1.2294  , p=0.3147  , df_denom=38, df_num=4\n",
      "ssr based chi2 test:   chi2=6.0823  , p=0.1931  , df=4\n",
      "likelihood ratio test: chi2=5.7197  , p=0.2211  , df=4\n",
      "parameter F test:         F=1.2294  , p=0.3147  , df_denom=38, df_num=4\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 5)\n",
      "ssr based F test:         F=1.9505  , p=0.1107  , df_denom=35, df_num=5\n",
      "ssr based chi2 test:   chi2=12.8174 , p=0.0252  , df=5\n",
      "likelihood ratio test: chi2=11.3066 , p=0.0456  , df=5\n",
      "parameter F test:         F=1.9505  , p=0.1107  , df_denom=35, df_num=5\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 6)\n",
      "ssr based F test:         F=1.5237  , p=0.2022  , df_denom=32, df_num=6\n",
      "ssr based chi2 test:   chi2=12.8561 , p=0.0454  , df=6\n",
      "likelihood ratio test: chi2=11.3084 , p=0.0793  , df=6\n",
      "parameter F test:         F=1.5237  , p=0.2022  , df_denom=32, df_num=6\n",
      "38.882, -76.958\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 1)\n",
      "ssr based F test:         F=4.2916  , p=0.0451  , df_denom=38, df_num=1\n",
      "ssr based chi2 test:   chi2=4.6304  , p=0.0314  , df=1\n",
      "likelihood ratio test: chi2=4.3871  , p=0.0362  , df=1\n",
      "parameter F test:         F=4.2916  , p=0.0451  , df_denom=38, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 2)\n",
      "ssr based F test:         F=1.9256  , p=0.1609  , df_denom=35, df_num=2\n",
      "ssr based chi2 test:   chi2=4.4015  , p=0.1107  , df=2\n",
      "likelihood ratio test: chi2=4.1757  , p=0.1240  , df=2\n",
      "parameter F test:         F=1.9256  , p=0.1609  , df_denom=35, df_num=2\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 3)\n",
      "ssr based F test:         F=1.2221  , p=0.3177  , df_denom=32, df_num=3\n",
      "ssr based chi2 test:   chi2=4.4683  , p=0.2151  , df=3\n",
      "likelihood ratio test: chi2=4.2303  , p=0.2376  , df=3\n",
      "parameter F test:         F=1.2221  , p=0.3177  , df_denom=32, df_num=3\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 4)\n",
      "ssr based F test:         F=1.1235  , p=0.3647  , df_denom=29, df_num=4\n",
      "ssr based chi2 test:   chi2=5.8887  , p=0.2076  , df=4\n",
      "likelihood ratio test: chi2=5.4747  , p=0.2420  , df=4\n",
      "parameter F test:         F=1.1235  , p=0.3647  , df_denom=29, df_num=4\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 5)\n",
      "ssr based F test:         F=0.8201  , p=0.5465  , df_denom=26, df_num=5\n",
      "ssr based chi2 test:   chi2=5.8356  , p=0.3225  , df=5\n",
      "likelihood ratio test: chi2=5.4187  , p=0.3669  , df=5\n",
      "parameter F test:         F=0.8201  , p=0.5465  , df_denom=26, df_num=5\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 6)\n",
      "ssr based F test:         F=1.6147  , p=0.1882  , df_denom=23, df_num=6\n",
      "ssr based chi2 test:   chi2=15.1637 , p=0.0190  , df=6\n",
      "likelihood ratio test: chi2=12.6544 , p=0.0489  , df=6\n",
      "parameter F test:         F=1.6147  , p=0.1882  , df_denom=23, df_num=6\n",
      "38.91, -76.986\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 1)\n",
      "ssr based F test:         F=0.1014  , p=0.7511  , df_denom=70, df_num=1\n",
      "ssr based chi2 test:   chi2=0.1057  , p=0.7451  , df=1\n",
      "likelihood ratio test: chi2=0.1056  , p=0.7452  , df=1\n",
      "parameter F test:         F=0.1014  , p=0.7511  , df_denom=70, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 2)\n",
      "ssr based F test:         F=0.0841  , p=0.9194  , df_denom=67, df_num=2\n",
      "ssr based chi2 test:   chi2=0.1808  , p=0.9136  , df=2\n",
      "likelihood ratio test: chi2=0.1806  , p=0.9137  , df=2\n",
      "parameter F test:         F=0.0841  , p=0.9194  , df_denom=67, df_num=2\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 3)\n",
      "ssr based F test:         F=0.0662  , p=0.9776  , df_denom=64, df_num=3\n",
      "ssr based chi2 test:   chi2=0.2204  , p=0.9742  , df=3\n",
      "likelihood ratio test: chi2=0.2200  , p=0.9743  , df=3\n",
      "parameter F test:         F=0.0662  , p=0.9776  , df_denom=64, df_num=3\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 4)\n",
      "ssr based F test:         F=0.0418  , p=0.9966  , df_denom=61, df_num=4\n",
      "ssr based chi2 test:   chi2=0.1918  , p=0.9957  , df=4\n",
      "likelihood ratio test: chi2=0.1915  , p=0.9957  , df=4\n",
      "parameter F test:         F=0.0418  , p=0.9966  , df_denom=61, df_num=4\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 5)\n",
      "ssr based F test:         F=0.1035  , p=0.9911  , df_denom=58, df_num=5\n",
      "ssr based chi2 test:   chi2=0.6154  , p=0.9873  , df=5\n",
      "likelihood ratio test: chi2=0.6127  , p=0.9874  , df=5\n",
      "parameter F test:         F=0.1035  , p=0.9911  , df_denom=58, df_num=5\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 6)\n",
      "ssr based F test:         F=0.6961  , p=0.6538  , df_denom=55, df_num=6\n",
      "ssr based chi2 test:   chi2=5.1639  , p=0.5230  , df=6\n",
      "likelihood ratio test: chi2=4.9772  , p=0.5467  , df=6\n",
      "parameter F test:         F=0.6961  , p=0.6538  , df_denom=55, df_num=6\n",
      "38.873, -77.009\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 1)\n",
      "ssr based F test:         F=5.4715  , p=0.0326  , df_denom=16, df_num=1\n",
      "ssr based chi2 test:   chi2=6.4974  , p=0.0108  , df=1\n",
      "likelihood ratio test: chi2=5.5886  , p=0.0181  , df=1\n",
      "parameter F test:         F=5.4715  , p=0.0326  , df_denom=16, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 2)\n",
      "ssr based F test:         F=2.2587  , p=0.1439  , df_denom=13, df_num=2\n",
      "ssr based chi2 test:   chi2=6.2550  , p=0.0438  , df=2\n",
      "likelihood ratio test: chi2=5.3685  , p=0.0683  , df=2\n",
      "parameter F test:         F=2.2587  , p=0.1439  , df_denom=13, df_num=2\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 3)\n",
      "ssr based F test:         F=1.7443  , p=0.2211  , df_denom=10, df_num=3\n",
      "ssr based chi2 test:   chi2=8.8959  , p=0.0307  , df=3\n",
      "likelihood ratio test: chi2=7.1548  , p=0.0671  , df=3\n",
      "parameter F test:         F=1.7443  , p=0.2211  , df_denom=10, df_num=3\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 4)\n",
      "ssr based F test:         F=0.6873  , p=0.6232  , df_denom=7, df_num=4\n",
      "ssr based chi2 test:   chi2=6.2843  , p=0.1789  , df=4\n",
      "likelihood ratio test: chi2=5.3007  , p=0.2578  , df=4\n",
      "parameter F test:         F=0.6873  , p=0.6232  , df_denom=7, df_num=4\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 5)\n",
      "ssr based F test:         F=3.6861  , p=0.1152  , df_denom=4, df_num=5\n",
      "ssr based chi2 test:   chi2=69.1142 , p=0.0000  , df=5\n",
      "likelihood ratio test: chi2=25.8619 , p=0.0001  , df=5\n",
      "parameter F test:         F=3.6861  , p=0.1152  , df_denom=4, df_num=5\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 6)\n",
      "ssr based F test:         F=0.6107  , p=0.7521  , df_denom=1, df_num=6\n",
      "ssr based chi2 test:   chi2=51.3011 , p=0.0000  , df=6\n",
      "likelihood ratio test: chi2=21.5593 , p=0.0015  , df=6\n",
      "parameter F test:         F=0.6107  , p=0.7521  , df_denom=1, df_num=6\n",
      "38.927, -76.98\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 1)\n",
      "ssr based F test:         F=0.4525  , p=0.5079  , df_denom=23, df_num=1\n",
      "ssr based chi2 test:   chi2=0.5115  , p=0.4745  , df=1\n",
      "likelihood ratio test: chi2=0.5065  , p=0.4766  , df=1\n",
      "parameter F test:         F=0.4525  , p=0.5079  , df_denom=23, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 2)\n",
      "ssr based F test:         F=1.8040  , p=0.1904  , df_denom=20, df_num=2\n",
      "ssr based chi2 test:   chi2=4.5099  , p=0.1049  , df=2\n",
      "likelihood ratio test: chi2=4.1462  , p=0.1258  , df=2\n",
      "parameter F test:         F=1.8040  , p=0.1904  , df_denom=20, df_num=2\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 3)\n",
      "ssr based F test:         F=1.2623  , p=0.3188  , df_denom=17, df_num=3\n",
      "ssr based chi2 test:   chi2=5.3462  , p=0.1481  , df=3\n",
      "likelihood ratio test: chi2=4.8266  , p=0.1849  , df=3\n",
      "parameter F test:         F=1.2623  , p=0.3188  , df_denom=17, df_num=3\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 4)\n",
      "ssr based F test:         F=1.0594  , p=0.4126  , df_denom=14, df_num=4\n",
      "ssr based chi2 test:   chi2=6.9617  , p=0.1379  , df=4\n",
      "likelihood ratio test: chi2=6.0818  , p=0.1931  , df=4\n",
      "parameter F test:         F=1.0594  , p=0.4126  , df_denom=14, df_num=4\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 5)\n",
      "ssr based F test:         F=0.3555  , p=0.8683  , df_denom=11, df_num=5\n",
      "ssr based chi2 test:   chi2=3.5550  , p=0.6151  , df=5\n",
      "likelihood ratio test: chi2=3.2954  , p=0.6545  , df=5\n",
      "parameter F test:         F=0.3555  , p=0.8683  , df_denom=11, df_num=5\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 6)\n",
      "ssr based F test:         F=0.5012  , p=0.7922  , df_denom=8, df_num=6\n",
      "ssr based chi2 test:   chi2=7.8939  , p=0.2460  , df=6\n",
      "likelihood ratio test: chi2=6.7013  , p=0.3494  , df=6\n",
      "parameter F test:         F=0.5012  , p=0.7922  , df_denom=8, df_num=6\n",
      "38.876, -77.003\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 1)\n",
      "ssr based F test:         F=0.4718  , p=0.4962  , df_denom=39, df_num=1\n",
      "ssr based chi2 test:   chi2=0.5081  , p=0.4760  , df=1\n",
      "likelihood ratio test: chi2=0.5051  , p=0.4773  , df=1\n",
      "parameter F test:         F=0.4718  , p=0.4962  , df_denom=39, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 2)\n",
      "ssr based F test:         F=0.0751  , p=0.9278  , df_denom=36, df_num=2\n",
      "ssr based chi2 test:   chi2=0.1712  , p=0.9180  , df=2\n",
      "likelihood ratio test: chi2=0.1708  , p=0.9181  , df=2\n",
      "parameter F test:         F=0.0751  , p=0.9278  , df_denom=36, df_num=2\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 3)\n",
      "ssr based F test:         F=0.2666  , p=0.8490  , df_denom=33, df_num=3\n",
      "ssr based chi2 test:   chi2=0.9695  , p=0.8086  , df=3\n",
      "likelihood ratio test: chi2=0.9579  , p=0.8114  , df=3\n",
      "parameter F test:         F=0.2666  , p=0.8490  , df_denom=33, df_num=3\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 4)\n",
      "ssr based F test:         F=0.0906  , p=0.9847  , df_denom=30, df_num=4\n",
      "ssr based chi2 test:   chi2=0.4713  , p=0.9762  , df=4\n",
      "likelihood ratio test: chi2=0.4685  , p=0.9765  , df=4\n",
      "parameter F test:         F=0.0906  , p=0.9847  , df_denom=30, df_num=4\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 5)\n",
      "ssr based F test:         F=0.1182  , p=0.9872  , df_denom=27, df_num=5\n",
      "ssr based chi2 test:   chi2=0.8319  , p=0.9750  , df=5\n",
      "likelihood ratio test: chi2=0.8229  , p=0.9756  , df=5\n",
      "parameter F test:         F=0.1182  , p=0.9872  , df_denom=27, df_num=5\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 6)\n",
      "ssr based F test:         F=0.0727  , p=0.9982  , df_denom=24, df_num=6\n",
      "ssr based chi2 test:   chi2=0.6721  , p=0.9951  , df=6\n",
      "likelihood ratio test: chi2=0.6660  , p=0.9952  , df=6\n",
      "parameter F test:         F=0.0727  , p=0.9982  , df_denom=24, df_num=6\n",
      "38.912, -77.013\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 1)\n",
      "ssr based F test:         F=1.5909  , p=0.2149  , df_denom=38, df_num=1\n",
      "ssr based chi2 test:   chi2=1.7165  , p=0.1901  , df=1\n",
      "likelihood ratio test: chi2=1.6816  , p=0.1947  , df=1\n",
      "parameter F test:         F=1.5909  , p=0.2149  , df_denom=38, df_num=1\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 2)\n",
      "ssr based F test:         F=0.5138  , p=0.6027  , df_denom=35, df_num=2\n",
      "ssr based chi2 test:   chi2=1.1744  , p=0.5559  , df=2\n",
      "likelihood ratio test: chi2=1.1575  , p=0.5606  , df=2\n",
      "parameter F test:         F=0.5138  , p=0.6027  , df_denom=35, df_num=2\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 3)\n",
      "ssr based F test:         F=0.5408  , p=0.6579  , df_denom=32, df_num=3\n",
      "ssr based chi2 test:   chi2=1.9771  , p=0.5772  , df=3\n",
      "likelihood ratio test: chi2=1.9286  , p=0.5873  , df=3\n",
      "parameter F test:         F=0.5408  , p=0.6579  , df_denom=32, df_num=3\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 4)\n",
      "ssr based F test:         F=0.7893  , p=0.5416  , df_denom=29, df_num=4\n",
      "ssr based chi2 test:   chi2=4.1369  , p=0.3878  , df=4\n",
      "likelihood ratio test: chi2=3.9268  , p=0.4160  , df=4\n",
      "parameter F test:         F=0.7893  , p=0.5416  , df_denom=29, df_num=4\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 5)\n",
      "ssr based F test:         F=1.4664  , p=0.2347  , df_denom=26, df_num=5\n",
      "ssr based chi2 test:   chi2=10.4338 , p=0.0638  , df=5\n",
      "likelihood ratio test: chi2=9.1914  , p=0.1017  , df=5\n",
      "parameter F test:         F=1.4664  , p=0.2347  , df_denom=26, df_num=5\n",
      "\n",
      "Granger Causality\n",
      "('number of lags (no zero)', 6)\n",
      "ssr based F test:         F=1.0357  , p=0.4280  , df_denom=23, df_num=6\n",
      "ssr based chi2 test:   chi2=9.7267  , p=0.1366  , df=6\n",
      "likelihood ratio test: chi2=8.6099  , p=0.1967  , df=6\n",
      "parameter F test:         F=1.0357  , p=0.4280  , df_denom=23, df_num=6\n",
      "38.864, -76.952\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Insufficient observations. Maximum allowable lag is 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-286-4c85036b7c96>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocations_ua\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtrial\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0malready\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m         \u001b[0mgct_cal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-273-9c9446c6aef4>\u001b[0m in \u001b[0;36mgct_cal\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;31m#running the GCT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                 \u001b[0mgct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrangercausalitytests\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/samuelstack/anaconda/lib/python2.7/site-packages/statsmodels-0.8.0-py2.7-macosx-10.5-x86_64.egg/statsmodels/tsa/stattools.pyc\u001b[0m in \u001b[0;36mgrangercausalitytests\u001b[0;34m(x, maxlag, addconst, verbose)\u001b[0m\n\u001b[1;32m    845\u001b[0m         raise ValueError(\"Insufficient observations. Maximum allowable \"\n\u001b[1;32m    846\u001b[0m                          \"lag is {0}\".format(int((x.shape[0] - int(addconst)) /\n\u001b[0;32m--> 847\u001b[0;31m                                                  3) - 1))\n\u001b[0m\u001b[1;32m    848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m     \u001b[0mresli\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Insufficient observations. Maximum allowable lag is 3"
     ]
    }
   ],
   "source": [
    "# For loop to runmy GCT function.\n",
    "for trial in locations_ua:\n",
    "    # looks to see if location has already been run.\n",
    "    if trial not in already:\n",
    "        gct_cal(trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Exporting to CSV\n",
    "granger_results_df.to_csv('gct_lag6_cvt.csv')\n",
    "granger_results_df.to_csv('gct_lag6_tvc.csv')\n",
    "#gct_lag6_cvt.csv, gct_lag6_tvc.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# ------------------------------------------------------------------------------------\n",
    "# Geopy\n",
    "This is my attempt to use Geopy's Geolocator to apply Geopy's unique location ID to locations rather than using my rounding meathod to create areas.\n",
    "\n",
    "Unfortunately it was very slow/ computationally expensive even when run on my AWS instance.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from geopy.geocoders import Nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "geolocator = Nominatim()\n",
    "\n",
    "local1 = geolocator.reverse('38.888110, -76.931799')\n",
    "local2 = geolocator.reverse('38.888111, -76.931800')\n",
    "local3 = geolocator.reverse('38.888000, -76.932000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55412802\n",
      "55412802\n",
      "55412831\n"
     ]
    }
   ],
   "source": [
    "print local1.raw['osm_id']\n",
    "print local2.raw['osm_id']\n",
    "print local3.raw['osm_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.dict[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'address': {u'city': u'Washington',\n",
       "  u'country': u'United States of America',\n",
       "  u'country_code': u'us',\n",
       "  u'house': u'Marshall Heights',\n",
       "  u'house_number': u'4907',\n",
       "  u'postcode': u'20019',\n",
       "  u'road': u'A Street Southeast',\n",
       "  u'state': u'District of Columbia'},\n",
       " u'boundingbox': [u'38.8880159', u'38.888188', u'-76.9319524', u'-76.93164'],\n",
       " u'display_name': u'4907, A Street Southeast, Marshall Heights, Washington, District of Columbia, 20019, United States of America',\n",
       " u'lat': u'38.88808215',\n",
       " u'licence': u'Data \\xa9 OpenStreetMap contributors, ODbL 1.0. http://www.openstreetmap.org/copyright',\n",
       " u'lon': u'-76.931801419496',\n",
       " u'osm_id': u'55412802',\n",
       " u'osm_type': u'way',\n",
       " u'place_id': u'77855796'}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local1.raw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OMS ID stays true down to the fifth decimol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------------------------------------------------------------------------------------\n",
    "# DBSCAN\n",
    "Similar to Geopy, 6 months of data was possible for DBSCAN  but, 1 year was too much data and would have taken over a day on my AWS instance as well.\n",
    "\n",
    "Running the 6 month DBSCAN was possible on my local machine however, calculating the Sillhouette score caused some serious issues on my machine killing my kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets see if we can create dbscan clusters and then reasign those back onto the list of values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for computations sake I am only going to take six months of data for the trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "theory_2 = theory[(theory['DATE'].dt.year == 2011)]\n",
    "theory_2 = theory_2[(theory_2['DATE'].dt.month <7)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = theory_2[['LONGITUDE','LATITUDE']]\n",
    "\n",
    "DB = DBSCAN(algorithm = 'auto', eps= 0.001, min_samples = 4 , metric = 'manhattan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dbs = DB.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated number of clusters: 474\n"
     ]
    }
   ],
   "source": [
    "core_samples_mask = np.zeros_like(dbs.labels_, dtype=bool)\n",
    "core_samples_mask[dbs.core_sample_indices_] = True\n",
    "labels = dbs.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "\n",
    "print('Estimated number of clusters: %d' % n_clusters_)\n",
    "\n",
    "\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, labels))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
